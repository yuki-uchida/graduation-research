{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['学生時代の友人7名で利用させて頂きました', 'ちょうど桜が咲いていて、別荘地内の桜もライトアップされていて、雰囲気良かったです',\n",
       "       '信州安曇野、穂高温泉郷に位置する本格ログハウスの貸別荘です', 'コテージ内はとても清潔感があって安心して泊まれました',\n",
       "       'インターから遠くもなく、コテージの近くにはコンビニや買い出しできるところもあるので、立地的にも満足でした',\n",
       "       '趣味で一棟貸しのところをいろいろ利用していますがこちらの設備環境はとても清潔で周辺もとても静かなところで居心地が大変良かったです',\n",
       "       '到着した日や翌日は雪の多い日でしたがスタッフの方が車や周辺の除雪をしてくれて大変助かりました',\n",
       "       'また時間があれば利用させていただきたいと思います', 'ちょうど紅葉が綺麗な季節に滞在できました',\n",
       "       'チェックインする前から、暖房がついていたりして、オーナーの気遣いを感じることができました',\n",
       "       'ベッドも、羽毛布団で、快適に眠ることができました', '高速のインターからも程よい距離で、車で簡単に買い物も行けて良かったです',\n",
       "       '今回は一泊二日だったので、次回は二泊くらいして、ゆっくり滞在したいです',\n",
       "       '夜はとても静かで、部屋も涼しくて、快適で、家にいる時よりも安眠でグッスリ眠りました',\n",
       "       '都会では目にすることができない綺麗な星空に家族全員感激していました',\n",
       "       'どこにどんなスーパーがあり、銭湯があり、レストランがあるのか、という地図も分かりやすいです',\n",
       "       '旅はいろんな場所に行きたいので同じ場所をリピートしたことがないのですが、ここは初めてまた行きたいと思えるところでした',\n",
       "       'スタッフ皆さんはとても親切で優しい方ばかりなのがここの素敵なとこです',\n",
       "       'コテージを一棟貸し切りで、お客様の別荘感覚でお使いいただけます。',\n",
       "       '1月10日から一泊スキー旅行のお宿として、仲間数名で利用させていただきました',\n",
       "       '予約する段階で、立地と薪ストーブに惹かれ即申込みました', '閑静な立地で、夜は星空がたいへんキレイでした',\n",
       "       '設備、サービス、案内等も丁寧に説明していただき感謝です', '仲間もすっかり気に入り、来年も同時期に伺えたらと思っています',\n",
       "       '子どもは、「また行きたい」と早くも言っております',\n",
       "       '娘と孫２人を連れての宿泊でしたが、部屋も眺めの良い高層階の海側を割り当てていただきました',\n",
       "       'プール等のレジャー施設も充実して、食事も豊富なバイキングで娘も孫も大変感激していました',\n",
       "       'おまかせプランだったのでどこの部屋になるか分からなかったけれど、フロントでHANA館の7階のDXツインと聞いた時には皆で大喜びしました',\n",
       "       'プールも温泉もボーリングもゲームセンターもお土産売り場も目の前のコンビニも全部あるので二日間飽きることなく満喫しました',\n",
       "       'スタッフの皆さんもフレンドリーでとてもいい休暇をすごしました！', '10年ぶりの家族旅行でしたが大変満足できる旅行となりました',\n",
       "       'アクアビートやガーデンなどの遊び場が充実しており、食事も和洋様々な種類があって、どれもこれも美味しかったです',\n",
       "       '夏は各コテージの庭でBBQや花火を楽しみ、冬はリビングに備わる薪ストーブで温まることができます。',\n",
       "       '12年旅行のプロが選ぶ５ツ星認定', '毎年恒例の杉乃井宿泊ですが、今回も満足しています',\n",
       "       '子供は思う存分プールで泳げるし、食事も満腹になるまで食べてしまうし、ホテルスタッフの対応も良いので、これからも利用させていただきます',\n",
       "       '夕食のブュッフェはどれも平均以下でした', 'お値段は少し高くてもハナ館で宿泊する事を強くオススメします',\n",
       "       '７月１８日に妻と娘と孫２人で宿泊しました',\n",
       "       '宿泊だけでなく、食事・温泉・プール・ボウリング場・ゲームセンター・その他イベントが素晴らしく、２日間みんなで楽しみました',\n",
       "       '「棚湯」って何？\\u3000と思っていたが、屋上に棚田のように湯ぶねが並んでおり、見晴らしも最高、開放感にもあふれ、とてもいい温泉でした',\n",
       "       'バイキングの食事も品数、味付けともに良かった', '杉乃井ホテルは横長の広いホテルなので、移動に時間がかかります',\n",
       "       'アクティブシニアの方もマイカーなしでリゾートライフを楽しむこともできます。 ',\n",
       "       '食事はバイキングですが、質が高いので、何を食べても満足でした', 'プールもあり、時間がいくらあっても足りないと思いました',\n",
       "       '海側のお部屋は景色はいいのですが、朝日がまぶしいことは知っておいた方がいいかもしれません',\n",
       "       '雨ですが、ボウリングなどをし、天気も気にせず楽しめました', '寝室でもログハウスを満喫することができます',\n",
       "       '子どもが裸足で結局お部屋をウロウロしたのが気になりました', '子ども用のスリッパが置いてあるとありがたいです',\n",
       "       '始めて、家族風呂を利用させて貰いましたが、清潔でとても気持ち良かったです',\n",
       "       'ご飯もとても美味しく、いつもは小食の子どももたくさん食べました',\n",
       "       'フロントさんやスタッフさんもとても親切で、楽しい家族旅行となりました',\n",
       "       '梅雨時の大雨の真っ最中、部屋は山側でしたが多分海側だとしても霧がすごかったのでどっちでも同じだったと思います',\n",
       "       '低階層とのことでしたが、グレードアップしていただいたのでしょうか、高階層でした',\n",
       "       'でも外の景色を堪能出来ないほど遊び疲れました（笑）霧がだいぶ深いせいか、却ってアクアガーデンのショーは綺麗に見えました',\n",
       "       '梅雨時は人も少なく、でも遊ぶのは屋内だし、棚湯も雨でもさほど気になりませんでした',\n",
       "       '思った以上に外国人スタッフが多く、海外から来る人には馴染みやすいのかもですね',\n",
       "       '開放的な三角の屋根の天井で、ログハウスを満喫できます。 ', 'この料金でここまでのサービスを受けられるのはお得だと思います',\n",
       "       'タオルを持ち歩かなくて良いのも楽だし、料理もバイキングなのに思った以上にクオリティ高かったです',\n",
       "       'BBQセットのレンタがあります。', 'アクアガーデンの更衣室の順路とかはもう少しわかりやすい方が良いと思いました',\n",
       "       'プール、お風呂、食事最高です！ホテルの対応も最高です！', 'ログハウスは全棟リビングに薪ストーブが設置してあります。',\n",
       "       'リニューアルされてとても綺麗なボーリング場で楽しめました', '大人４名と3歳2歳1歳の３名の子供連れで始めて利用しました',\n",
       "       '泊まってみて、このホテルの評価の高さが頷けました', 'まず、これだけたくさんの宿泊客がいて色々とスムーズだった事です',\n",
       "       'お部屋もとても清潔でこのホテルの節々に、従業員の方々の意識の高さを感じました',\n",
       "       '子供連れでのバイキングやバス移動など不安もありましたが、いざ行ってみると行き届いたサービスで不便さを感じる事はありませんでした',\n",
       "       '何台も乗ったバスの運転手さん達も毎回きちんとアナウンスしてくれてとてもいい印象で、安心して乗る事が出来ました',\n",
       "       'お風呂やプールについても小さい子供と一緒に楽しめる所が少ない中で、たくさん思い出が作れて本当に嬉しいです',\n",
       "       'とにかく褒める所ばかりで、ぜひまた利用したいと思えるホテルでした',\n",
       "       'その分、混雑は避けられないので今後その対策にまた期待しております',\n",
       "       '館内の連絡通路が工事中で通れませんでしたが、シャトルバスが次々に来るので全然不便ではありませんでした',\n",
       "       '今回、アクアビートを楽しみたかったので2泊しました', 'アクアビートは天気を気にせず楽しめて、5才の娘も大喜びでした',\n",
       "       'アクアガーデンは水着着用なので、主人も一緒に家族で温泉に入れて良かったです',\n",
       "       '私はスパも利用しましたが、施術中はキッズランドを無料で利用できたので、主人と娘も待ち時間を退屈せずに過ごせました',\n",
       "       'とても楽しい時間を過ごす事ができたので、また泊まりに行きたいです！',\n",
       "       '金曜日に宿泊したので費用対効果は今まで泊まったお宿の中で一番感じたホテルでした',\n",
       "       '雨が降っても楽しめますね！プールに棚湯にボーリング！夜は噴水ショーにプロジェクションマッピング！これでもかー！という位楽しませて頂きました',\n",
       "       '１つだけ期待が大きすぎてＧＡＰを感じたのはお食事です', '和食で好きなメニューがオプション扱いだらけだったのが残念です',\n",
       "       'オプションつけなくてもお食事を楽しめますが、いわゆる普通のブッフェに感じました',\n",
       "       '外国のお客様が多い為か和食が少なく感じました', '品川駅前で利便性が良く、度々、利用させて頂いています',\n",
       "       '駅からすごく近いように見えますが、前庭の分、実は少し距離あって、灼熱の中を泳いでたどりついた感じでした',\n",
       "       '新潟方面を回ってやってきて用向きは渋谷、帰宅は羽田だったので地理的に品川が便利で決めています',\n",
       "       'せっかくのマッサージチェアでしたが、なんとなく使いませんでした',\n",
       "       '丁度台風接近の最中お手数おかけしましたが、こちらに宿泊出来たこととても感謝しています',\n",
       "       '私も機会がありましたら是非とも利用させて頂きたいと思いました', 'また利用したいと思える素晴らしいホテルでした',\n",
       "       '立地も良く、サービスも良く、店内のレストランやコンビニもOK',\n",
       "       '快適に宿泊させて頂きましたが・・・部屋に携帯の充電器を忘れてたと思いますが？また、連絡をさせて頂きます',\n",
       "       'いつも出張で宿泊していて素晴らしいので、今回は都内に住む孫と会うために利用しました',\n",
       "       'チェックインの時に、孫のためにタオルもご用意いただき、そのご配慮に感動しました',\n",
       "       '部屋からの眺望も素晴らしく、孫はずっと外を見てました',\n",
       "       '立地が良いので、少し休んでから水族館に行ったり、猛暑のなかでも楽しく過ごせました',\n",
       "       '今回は出張利用だったので寝るだけでしたが、広い部屋で開放感があり、大きな窓からの朝日で気持ち良くスッキリ目覚めました',\n",
       "       'アクセスが良いので仕事ギリギリまでゆっくりできるのも良いですね',\n",
       "       '設備は多少古くなっていますが、メンテナンスや清掃も行き届いている感じです',\n",
       "       'とても大きいホテルなので、フロントやレストランには人が多いですが、客室はとても静かでくつろげました',\n",
       "       '駅が目の前で、駅からフロントまで階段もないので、重い荷物があってもあまり苦になりませんでした',\n",
       "       '翌日は、仕事でシングルを予約していましたが、「お部屋変え」ということで、チェックアウトを12時してもらえました',\n",
       "       '多少の古さは感じるものの、この場所、この価格でこのクオリティはすごいですね', 'これからもぜひ利用させていただきたいと思います',\n",
       "       '何より品川駅からすぐで交通の便が良く、部屋も広くて朝食が美味しい',\n",
       "       '少し古さはありますが、それに見合った価格設定になっています',\n",
       "       '立地も抜群！今まで宿泊したホテルの中でコストパフォーマンスに優れていると感じました',\n",
       "       'たいへん高級感があって過ごしやすかったです', '空調等設備は古いですが、問題はなかったと思います',\n",
       "       'スリッパやボディーシャンプー類等、アメニティーはとてもいいものでしたし、なんといってもベッドが広く快適で、よく眠れました',\n",
       "       '品川駅に着き 高輪口 出てすぐにわかりました', 'ホテル周辺の環境もよくまた 利用したいと思いました！',\n",
       "       '建物は綺麗で気分良かったのに、ホテルを出てからがっかりしました', 'フロント対応もよく、部屋もきれいで、落ち着いて泊まれました',\n",
       "       '本当に国内で一番豪華じゃないかと思うビュッフェを楽しませてもらいました',\n",
       "       'まぐろの解体ショーも楽しく、ローストビープも美味しかったです',\n",
       "       'セミダブルのお部屋でゆったり眠ることが出来ました！お部屋はとてもきれいです', 'とっても可愛くて思わず話しかけてしまいました',\n",
       "       '17階のラウンジは、カフェのような空間でくつろぐことが出来ました！ありがとうございました！',\n",
       "       'アネックスタワーとメインタワーで全部楽しめます',\n",
       "       '怪しい中国人が「部屋に忘れてきた」とか言ってすり抜けてたから、狡いなって、思ってました',\n",
       "       'ただ、席指定なのに、勝手に人の前に座ってくる外人が多くて困ります',\n",
       "       '仲間で来ておきながら、別々に入場するからだと思うけど、その辺は、伝わっていない気がします',\n",
       "       'でなければ、専用の待合席を設けて、揃ったら優先してテーブルに案内するとか',\n",
       "       '関東圏に住む息子に孫が生まれたとの報に接し、急きょ宿をとる必要にせまられた', '楽天で品川地区を検索、本プランを見つけ即予約',\n",
       "       '偶々以前アネックスに宿泊し、夕食にハプナも利用', '今回は品川ブリンス2回目ですがとても満足しています',\n",
       "       'Nタワー１７階で妻とゆっくり、ゆったり過ごさせていただきました',\n",
       "       'お掃除スタッフの方が海外からのお客様に丁寧に対応されていたのをみて、素敵だと思いました',\n",
       "       '当日大変込み合っていましたが、スタッフの連携がよかった',\n",
       "       '数年ぶりに宿泊しましたが、チェックインが自動で出来るようになっていたのが便利でよかったです',\n",
       "       '代理で予約しましたが、とても喜んでおりました', '代理で予約しましたが、とても喜んでおりました',\n",
       "       '一人で宿泊しましたが、キーがないと宿泊階にいけない仕様で、安全性に長けていました',\n",
       "       'リニューアルされた館内はとても清潔感があり、朝食も簡単なブッフェでしたが十分でした',\n",
       "       '今回は忘れ物をしてしまい、丁寧に取り計らっていただきました', 'サービスも良く気持ちよく利用できました次回も使いたい',\n",
       "       'Ｎタワーを何度か利用しておりますが毎回快適に過ごせます',\n",
       "       '今回、デリバリーロボットさんに栓抜きを持ってきて頂きましたが、可愛いいいこです(´・∀・)また宿泊し利用したいです',\n",
       "       '初めてPARK WINGルームに宿泊しましたが、期待以上のお部屋で家族全員大満足でした',\n",
       "       '娘らは前泊のディズニーホテルより断然良いと大絶賛でした',\n",
       "       '孫たちの誕生日の無料サービスをお願いしたところ、爺は涙が出るくらい感激感動しました・・・本当にありがとうございました',\n",
       "       '最後に、帰りのリムジンバス乗り場の女性スタッフの孫たちへの対応が素晴らしかったです・・・感謝！！！',\n",
       "       'チェックインでは時間がかかりましたが朝食もとてもおいしくまた利用したいと思いました',\n",
       "       'レストランの食事も美味しく、サービスも心地よく、部屋からの眺めも良く、本当に気持ちよく滞在させて頂きました',\n",
       "       '一つ、ベッドですが確かにとても寝心地良く、子供も熟睡していましたが、高さがとても高く、添い寝する子供が落ちないかとヒヤヒヤしました…',\n",
       "       '犬を預けることができ、安心してディズニーランドで遊ぶことができ、部屋でも犬と楽しく過ごすことができました',\n",
       "       'フロントも高級感があり、チェックイン時に朝食で混みやすいレストランを先に教えていただきました',\n",
       "       'ディズニーランドに行くために1泊で家族で利用しました', '子供が小さいので、早めにチェックインして、休憩',\n",
       "       'あいにくのお天気でしたが、クラブフロアだったので、オアシスパスを利用して、プールで遊ぶことも出来て子供は大喜びでした',\n",
       "       'ラウンジで夜はカクテルタイムを、朝は朝食を取りました',\n",
       "       '建物は古いですが、掃除も行き届いており、洗面所の換気扇フィルターもきれいで安心しました',\n",
       "       'クレンジングなどが一切なく、ちょっと困りました', 'ここは、唯一改善をお願いしたいところです',\n",
       "       'それでも駐車場代も含めてこのお値段とクオリティ', 'スタッフも皆さん笑顔で挨拶してくださり、また、宿泊したいと思います',\n",
       "       'ホテル全体が綺麗で広く、大変過ごしやすかったです', 'ディズニーランドに来た際にはまた宿泊しようと思います',\n",
       "       '前日に旅行が決まっての予約でしたが、限定の格安プランを利用することが出来ました',\n",
       "       'お風呂はほかの方の口コミもあるように掃除がいき届いてはおらず洗い場スペースの汚れが気になりました',\n",
       "       '湿気もこもりやすく換気が弱いのかな？と感じました',\n",
       "       '朝食は会場を選ぶことが出来、今回はすぐ案内ができるという和食レストランを利用しました',\n",
       "       '子供用にパックの野菜ジュースがあったり、オムレツではなくだし巻き玉子を鉄板で焼いて提供してくれたり、サービスも良かったです',\n",
       "       '落ち着いた大人の朝食時間を過ごすにはいいのかな、と感じました',\n",
       "       '我が家は子供がまだ幼いので、並んでも1階の方が選択肢も多そうで子供も楽しめそうです',\n",
       "       'フロントやルームサービスの対応も良かったです', '朝食＆駐車場無料のプランで3歳の息子を連れ家族3人でお邪魔しました',\n",
       "       '3歳の息子は楽しんでいた様です', 'お部屋も快適で、何より窓から花火が綺麗に見えたのに感動しました',\n",
       "       '小さい子供がいるので洗い場付きのお風呂や貸し出しの踏み台等助かりました',\n",
       "       '翌日は朝食を美味しくいただいた後に、オアシスで遊んで帰路につきましたがアスレチックやボールプール等息子は大満足の様子でした',\n",
       "       'ＴＤＲの後泊として利用させていただきました', '朝食も良かったですよ（ハワイのそこと比べてしまうとちょっと負けますが）',\n",
       "       'サービスのフライドライスですが、厨房のほうで予め調味をしていただいたほうがおいしくできるような気がします',\n",
       "       'わがままを言えばデイズニーリゾートラインは結構短距離なわりにお高いのでシャトルバスがそこまで行ってくださると満点です',\n",
       "       '息子の1歳の誕生日祝いで前日に宿泊しました',\n",
       "       '掃除が行き届いたきれいな部屋で、サービスで置いてある紅茶やコーヒーが今まで泊まったホテルとは比べ物にならないくらいおいしかった',\n",
       "       'フロントの方も感じが良くまた利用したいと思います',\n",
       "       '個人的には過去何度か宿泊していますが、家族でグランカフェの朝食を取りたくて宿泊しました',\n",
       "       '大浴場が別に用意されているともっと良かったな～と思いました', 'すごく素敵な部屋で、子供達も大興奮していました',\n",
       "       '布団を用意して下さった方達が明るくて、子供が喜んでいました',\n",
       "       '手洗いせっけん置きが、陶器だったので、子供に扱わせるのが怖かったです', '二泊三日でオーシャンドリームルームの部屋でした',\n",
       "       '子どもが生まれてからDisneyに行く時は毎回利用させて頂いています', 'ベットが広くて添い寝するには非常に良いです',\n",
       "       'キャンセル料もギリギリまでかからないので子どもの体調によって変更も出来て助かってます',\n",
       "       'そして、私は朝食ブッフェが好きで内装も可愛いし、料理も美味しいです',\n",
       "       'モノレールの駅からはバスを利用せず、徒歩で行きましたが、とても近くて便利でした',\n",
       "       'お部屋はダブルベットをくっつけて利用できたので、子どもが落ちるかも、などの心配をせずぐっすり休めました',\n",
       "       'ただ朝食はフロント近くのカフェのプランだったため、種類が少なく残念でした',\n",
       "       '次回は朝食なしのプランで、舞浜駅やパーク内で食べるのもありかなぁ、と思いました',\n",
       "       '今後はディズニーに行くならこのホテルか、ミラコスタ、どちらかにしようと思います！',\n",
       "       '久しぶりのお泊りディズニーで利用させていただきましたがほんとにお手頃な価格で、それだけで満足でした', '女性の方でも安心です',\n",
       "       '2人での利用なのでお部屋もお広く清潔感もありゆったりと過ごすことができました、また利用させていただきたいと思います',\n",
       "       '当日、禁煙ルームへの変更をお願いしたところ12階にあるとても広い部屋に案内していただきました',\n",
       "       'キングサイズのベット、リビングルーム、トイレ、テレビも二ヶ所あり、大満足でした',\n",
       "       '毎月２日間、友人とディズニーを楽しんでおります', 'ホテルまでは駅からも徒歩で５分くらい行けるので便利です',\n",
       "       '送迎バスも有るので当日の疲れ具合や天候を加味して利用しております',\n",
       "       'ホテル内にコンビニも有るのでお水等を購入することも出来て便利です',\n",
       "       '今回はスタンダードの予約でしたが…キャンセルが出たようでグレードアップしていただきました',\n",
       "       '駐車場込みだったので、とても良かったです', 'また、こちらのホテルを利用したいと思います',\n",
       "       'また、そのケーキを持ってきてくださった方の対応がとても紳士的で素晴らしく感激しました',\n",
       "       'お部屋も三人部屋でベッドもすべて大きくて寝心地は満点です',\n",
       "       'お風呂は窓があり海が見えて、洗面を挟んでシャワー室があり素敵でした',\n",
       "       '9階の部屋で、バルコニーから下を見たら綺麗なプールが見えました', '朝10時から多くの方々が利用されていました',\n",
       "       '吹き抜けになっていて、とても解放感があり、明るく、チャペルなども上から見えて、とても贅沢な作りだなと思いました',\n",
       "       '久しぶりのクラブリゾートでしたが、やっぱり落ち着きます', '部屋とバスルームは、十分なほどの広さがあります',\n",
       "       'ピンクの外観が可愛いのと吹き抜けの明るさと開放感も気に入ってます', 'スタッフの皆さんも感じがいいので、また泊まりたいです',\n",
       "       'ディズニーシーのパイレーツサマー、楽しかったです',\n",
       "       'ディズニーランドの後泊で利用しました！３回目の利用でしたがいつも変わらず「使って良かった！」と思わせて頂いてます',\n",
       "       '当日部屋のグレードアップを無料でして頂けお部屋に大満足でした',\n",
       "       '朝食もバイキングを頂きましたが、子供用にカレーを甘くして頂けたり気づかいが素晴らしかったです',\n",
       "       'オフィシャルホテルの中で、全ての面において安心して予約できます',\n",
       "       '今回、初めて修学旅行生と同宿でしたが、一般客との同線が重ならないようきちんと配慮されていてさすがだと思いました',\n",
       "       'スタッフさんが一人一人の対応、笑顔が素敵でした', '私のお気に入りのホテルとして使わせて頂いております',\n",
       "       'お部屋から見えるランドやシーの景色を眺めながら、気分は最高にMAX!!私にとっては充電の場所となってるみたいです',\n",
       "       'ただ、一つだけ気になることがありましたのでコメントさせて頂きます',\n",
       "       'スリッパなのですが使い捨て用のものがあるととっても嬉しいです',\n",
       "       'ディズニー周辺で喫煙のお部屋を探していてお値段手頃だったこちらに決めました',\n",
       "       'ホテルでは寝るだけなので期待していませんでしたがフロントの方も丁寧で、グレードアップしていただけた事も嬉しかったです',\n",
       "       'またお泊まりディズニーする際は利用したいです', 'ディズニーランドに行く為に、家族3人での女子旅でした',\n",
       "       '朝食バイキングは、品数も豊富でシェフの手作りオムレツは とても 美味しかった',\n",
       "       '洗面所とバス、トイレが別なので 朝の混雑も 快適に過ごしました',\n",
       "       '舞浜駅から、シャトルバスで  5分の近さ、運転士さんの  トークも 楽しく  あっと言うまに ホテルに到着します',\n",
       "       '部屋の広さは  トランクも広げられて  ちょうど よいですね！ ', '部屋も広く、パークが一望できるお部屋にしていただきました',\n",
       "       'とても綺麗でゆっくりできたのでまた利用したいです！', '舞浜駅に行くには20分おきに無料バスがでていて便利でした',\n",
       "       '前から予約させてもらってたからか豪華なお部屋になってて嬉しかったです！', 'リーズナブルな価格で泊まれてとても満足です',\n",
       "       '立地がよく、電車で降りてから荷物を預けてユニバに行けたのがとても便利でした',\n",
       "       'ホテルのシャンプー、リンスも使い心地が良くてよかったです', 'チェックアウト後に朝からusjに行きました',\n",
       "       '荷物をロッカーに預けなければならず、探しに行こうと思っていたところ、チェックアウトしたにも関わらず、無料で預かってくれてとても助かりました',\n",
       "       '予約した日付を間違えて訪れ、夕食の用意まで親切に対応して頂きましてありがとうございます', '天城山の麓に位置する旅館',\n",
       "       'オープンしたてでとても綺麗でした！スタッフさんが声をかけてくださって、快適に過ごすことができました！',\n",
       "       'ダイビングのため一人でカプセルルームに宿泊しました温泉は源泉かけながしなので循環のお湯にありがちな臭さも全くなく快適に入浴できました',\n",
       "       'この価格で大きなお風呂を利用できるのはありがたかったです', '食事は文句なしに美味しく満足できるものでした',\n",
       "       'プレステとニンテンドーのゲーム機があります', '東伊豆でダイビングすることが多いのでまた利用するつもりです',\n",
       "       '温泉あり、夕飯朝食あり、広々とした部屋ありとってものんびり過ごせました！ありがとうございました！',\n",
       "       'この値段で、沢山の品揃え、アルコールまで飲んで充分満足しました',\n",
       "       '床で滑ってコーヒーをこぼしても、丁寧にすぐ綺麗に拭いて頂き、気持ち良かったです',\n",
       "       '卓球・カラオケ・麻雀・囲碁・・・全て無料とは驚きです', 'まるで昭和の時代にタイムスリップしたようでした',\n",
       "       'けっこうお年を召した従業員の方々も頑張って働いていらして、いいホテルだとしみじみ思いました',\n",
       "       'ゴールデンウィークに宿泊混んでいたけど食事の補充などは早くとても満足でした', '花火大会開催日の宿泊は無駄に高いホテルが多い',\n",
       "       '蟹の食べ放題を味わいたい！', '値段が安すぎるので不安だったけど、蟹を美味しく頂きました！',\n",
       "       'な大人平日一泊二食で９千円くらいです', 'カラオケ無料、全自動麻雀無料、卓球無料',\n",
       "       '非常にコストパフォーマンスの良いホテルでした', 'ここは何度か利用しましたが、露天風呂が断トツに素晴らしいです',\n",
       "       '広いし、なんといっても露天風呂に水風呂があるのが最高に良いです',\n",
       "       '湯巡りで熱海の系列ホテルのお風呂に送迎バスで無料で廻れるのが魅力です',\n",
       "       'ご飯も美味しく、部屋もとても快適で、とても充実した宿泊になりました', '駅から徒歩圏内である',\n",
       "       '本館1Fと別館の風呂も入りましたが、展望露天風呂が一番気持ちよかったです',\n",
       "       'すぐ近くにコンビニがあり、徒歩圏内に食事処、居酒屋もあり便利です', '古き温泉宿と考えれば和室も懐かしく許容範囲かなぁ',\n",
       "       '若かりしころをおもいだして、合宿感覚で楽しめるように全員が同じ部屋に宿泊できるホテルを探していました',\n",
       "       'このホテルは雑誌にも特集が載っていた', '立地もお土産屋、飲食店が点在する通りからも、海にも近く非常によかったです',\n",
       "       '６歳の長男は時折来る海賊船を部屋から見つけては歓声を上げていました',\n",
       "       '値段もリーズナブルでまた利用したいねと口をあわせて帰路につきました',\n",
       "       '遅い時間の到着になりましたが親切に対応していただきました', '食事は、お刺身、豪華お寿司、フェアという割には普通でした',\n",
       "       '施設の老朽化は否めないが、金額に対して施設、サービス、食事、総てに於いて満足です',\n",
       "       'お風呂は３つあり、宿泊日が平日だったのでゆったりとできました', '客室が沢山あるので混雑する週末には一抹の不安があります',\n",
       "       '展望風呂からの日の出は最高の贅沢でしたょ', 'この１年間に3回も行ってるリピーターです',\n",
       "       'お部屋は寝るためだけのもので寝具が清潔でさえあればと考えておりますので、このままで値上げしないでください',\n",
       "       '偏食の小学生の娘はバイキングで食べられるものだけを食べられるこのホテルがお気に入りです',\n",
       "       '私がこんなにリピーターになっているのは、なんといっても蟹です',\n",
       "       '３０分おきに駅から無料送迎バスも出ていますが、歩いて１５分ほどです',\n",
       "       '熱海ビーチまでも歩いて５分～１０分ほどなので、夏は最高です',\n",
       "       '食事の種類がとても多く、また一つ一つ全部美味しかったため大満足です！お会計の際も細かな気配りがあり丁寧でした',\n",
       "       '料理はA５ランクの牛肉とキンメの煮付けを追加注文満足できました',\n",
       "       '記念日のケーキが冷蔵庫に入っていることが案内になかったことがしいていえば残念',\n",
       "       '飲み放題、食べ放題、カラオケ無料、ダンスホール無料、卓球無料とホテル代以外のことにお金を使わずに済むのはありがたかったです',\n",
       "       'スマホの充電器を忘れてしまいフロントで充電もしていただき本当に助かりました！', '駅からバス送迎のプランがある',\n",
       "       'お寿司の所にいた女性がとても気を使ってくれたのがとても嬉しかった',\n",
       "       'チェックイン時に部屋をジュニアスイートにグレードアップさせて頂きました…と部屋に入ると確かに綺麗で広いお部屋でした',\n",
       "       '部屋は良かったのですが上か隣の部屋に子供がいたのか、物音が遅くまでして少々寝不足でした',\n",
       "       '食事はブッフェスタイルで、どれを食べても美味しく満足、特にステーキや金目鯛の焼物は何度でもお代わりができテーブル迄持って来てくれます',\n",
       "       '部屋の露天風呂が温泉でないのは残念でした', '小田原からバス20分で宿に到着します',\n",
       "       'お風呂は洗い場が仕切りがあり、隣を気にせず、利用出来ました', '夕食、朝食共に種類が豊富で、美味しく食べすぎてしまいました',\n",
       "       'お部屋にお風呂がないプランでしたが、シャワーブースはありました',\n",
       "       '大浴場にはダイソンのドライヤーやナノイースチーマーも置いてあり、新しいホテルらしい設備でした',\n",
       "       '食事も適度な種類で満足感のある質のバイキングでした',\n",
       "       'お肉と魚は出来立てをサーブしてもらえ、デザートも種類は多く満腹になるまで楽しめました',\n",
       "       '5年後10年後もこのクオリティであってほしいと思いました', 'また箱根へ行く際には行ってみたいと思います',\n",
       "       'とても清潔感のある綺麗な部屋で大変満足しました！', '夜ご飯が90分制なのが少し残念でした',\n",
       "       '美味しいからこそもっと食べたいなとも思いました', '全体的に満足だったので、もう一度来たいです',\n",
       "       'また違う友達、家族でも宿泊したいと思っています', '桃源台のすぐ近くの立地でとても便利でした',\n",
       "       'あいにくの天気でしたが、ビュッフェの食事が朝夕ともよく、一緒に行った両親も満足しておりました',\n",
       "       '立地も建物も食事もサービスも満足です',\n",
       "       '大浴場の終了時間の１時間も前から、入り口に掃除中の立て札が立てられて掃除人が出入りしている',\n",
       "       'ゆっくりできなかったので、もう来ないと思います', 'チェックアウト時間も新しいホテルにしては早い１０時だった',\n",
       "       'ブッフェスタイルでの食事でしたが、非常に美味しかったです', 'また、スタッフのサービスやお風呂も大変満足しました',\n",
       "       '家族四人で行きました！', '赤ちゃん連れにとって、お部屋が清潔であることはとてもストレスフリーで快適でした',\n",
       "       'お部屋の露天風呂やフカフカのベッドを次男が大変喜んで、キャッキャと終始ご機嫌でした',\n",
       "       'カラオケや麻雀が無料で使用できたこともお得感を感じることができる',\n",
       "       '食事も子連れにとってブッフェは好きなものを食べられるので良かったです',\n",
       "       '次男は90分間もくもくと大人しく食べ続け、おかげで大人もゆっくり食事を満喫できました',\n",
       "       '従業員の方々もみなさん感じがよく、すれ違うたび挨拶して下さり気持ちがよかったです',\n",
       "       '欲を言えば、チェックアウトが11時だったらさらにゆっくりできたかなぁと思います', '本当に宿泊してよかったと思えたお宿でした',\n",
       "       'サービス、館内、食事、お風呂、アメニティなどすべてが期待以上でした',\n",
       "       'まずまだ新しいお宿ということもあり、とても館内がきれいです', 'そして水盤テラス？はとてもよい写真が撮れそうでした',\n",
       "       'また、足湯があり、そこからは芦ノ湖の景色がとてもきれいに見えました',\n",
       "       '食事についてですが、友人と4人で利用したのですが、4人が4人ともおいしいと思える味でした',\n",
       "       'それぞれのお料理も安っぽいものではなく、工夫されていたりして見た目が良いだけではなく味も素晴らしいものでした',\n",
       "       '朝も利用しましたが、美顔器を朝から使えて嬉しかったです',\n",
       "       'お風呂から出たところに冷煎茶と冷水が置いてあったことも有難いなあと思いました',\n",
       "       'アメニティについてはDHCのもので女性にとってはとても助かるし嬉しいなと思いました',\n",
       "       'そして館内を浴衣とサンダルで歩けることが良いと思います', 'また館内用の浴衣のデザインも素敵でオシャレだなと思いました',\n",
       "       'そして館内を移動する際のバッグに困らないようにとの配慮からか、カゴバッグも部屋内に置いてあり、使わせていただきました',\n",
       "       '部屋からの景色も湖側だったからか素晴らしかったです', 'バルコニーに椅子とテーブルがある',\n",
       "       '話しかけたくれたり、お食事会場ではあいたお皿を言う前に下げていただけたり、素晴らしかったです', 'とにかく素晴らしかったです',\n",
       "       'とにかくよかったです', '一緒に行った友人からもここを予約してくれてありがとうと言われました',\n",
       "       'もし今後また箱根に行く機会があれば絶対にこちらのお宿を利用させていただこうと思います',\n",
       "       'フロントから芦ノ湖を眺める庭一面に水が張ってあり、芦ノ湖にそのままつながっているような絵になります',\n",
       "       '水面の中央にサークルテーブルが設けられており、絶好のフォトスポットになってます',\n",
       "       '足湯に浸かりながら、お酒や飲みのを楽しむことも可能',\n",
       "       'ビュッフェスタイルのお料理も、一つ一つ手がかけてあり、とても美味しく満足でした', 'ホテルはとても綺麗で、大満足です',\n",
       "       '繁華なエリアから敢えて離れていながら、渋滞する車を置いて多様な移動方法をとれる、とても気の利いた立地です',\n",
       "       '親戚を箱根に招待する時は、また是非利用したいです',\n",
       "       'オープン一周年ということでしたが、ぜひとも人気を衰えさせず、流行りやオシャレに敏感なままの、素敵なホテルであり続けて欲しいです',\n",
       "       'またお伺いしたいと思いました', '到着時にお出迎えをしてくださって荷物のカートをすぐに用意していただき期待が高まりました',\n",
       "       '中に入ると本当にきれいで素敵な宿でした！フロントの対応もとても気持ちが良かったです',\n",
       "       '5歳と1歳の子連れでしたので夕食の時間を早くしていただきありがとうございました',\n",
       "       'あまりにも気に入って延泊をお願いしたら気持ち良く対応していただけました',\n",
       "       'また子連れなのでベッドをくっつけるように事前にお願いしておきましたが対応していただけました',\n",
       "       'お料理はどれも美味しくて見た目もキレイで大満足でした', 'チョコバナナがあって子どもが喜んでいました',\n",
       "       '貸切風呂にも大浴場にもバスタオルの用意があるのでとても便利でした', 'どのスタッフも気持ちの良い挨拶をしてくれる',\n",
       "       '5歳の娘が宿をとても気に入ってここに住みたい帰りたくないと泣いていました',\n",
       "       '次の旅行に泊まりたくて\\u3000早めに予約しました', 'スタッフのみなさんとても親切だし\\u3000お料理も大満足です',\n",
       "       '大きな窓から見える景色を楽しみながらコーヒーを飲んで・・・最高でした！',\n",
       "       'お値段もお安く、ご飯も種類が豊富でたくさんいただきました',\n",
       "       '上の子が食事付、寝具なしを選んだんですが、お部屋にタオル、歯ブラシがなかったのが残念でした',\n",
       "       '部屋も、きれいにされており、目立つ汚れはありませんでした', 'クロスの汚れを張り替えた跡がありましたが、全く問題ないです',\n",
       "       '朝食に金目のあら煮が置いてありましたが、食べるところがほとんどない状態でした', 'それでも、朝夕ともに満足のいく内容でした',\n",
       "       '金目鯛のお刺身がドーンと舟盛りになっていて、息子は大興奮でした！',\n",
       "       '夜に息子がロビーで走って転んだのですが、フロントにいた職員の方たちが直ぐに気づき声を掛けていただきました',\n",
       "       'また機会があればお世話になりたいと思っています',\n",
       "       '料理が楽しみで山形から車で行っております！いつもは妻と、今回は妻の姉を誘い3人で10月15日と16日の連泊で行ってきました',\n",
       "       '１０月１１日に家族で利用させていただきました', '楽天スーパーSALEは、とてもいいプランでした',\n",
       "       '値段は安く、料理の種類も豊富で満足でした', '部屋はやや古くても二間で使いやすかったです',\n",
       "       'ホテルの立地的な問題かもしれませんが、大浴場の風景はなく、まるでビルの谷間にあるイメージでした',\n",
       "       '宿泊代が安かったので、正直すべてにおいて期待していませんでした',\n",
       "       'が！！ホテル入口で駐車場の案内をしていたかた、駐車場で車の誘導をしていたかた、フロントのかた、もうみなさんが、にこやか、かつ的確でプロでした',\n",
       "       'お部屋も広くて、お掃除も行き届いていましたし、あちこちにエアコンがあって暑がりな私たちも快適にすごせました',\n",
       "       '平日でしたが、土日のもっと宿泊客がいるときと変わらないものを出してくれているんだと思いました',\n",
       "       'お風呂もきれいでしたし、シャンプーが選べたりして楽しかったです',\n",
       "       '夏に私たち夫婦にとって大変つらいことがあり、元気になるための旅行でもありました'], dtype=object)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data1 = pd.read_csv(\"./input_data_1.csv\",header=None).drop(0,axis=0).drop([0],axis=1)\n",
    "data2 = pd.read_csv(\"./input_data_2.csv\",header=None).drop(0,axis=0).drop([0],axis=1)\n",
    "\n",
    "texts = np.concatenate([np.array(data1.iloc[2,:]),np.array(data2.iloc[2,:])])\n",
    "labels = np.concatenate([np.array(data1.iloc[1,:]),np.array(data2.iloc[1,:])])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    wakati = MeCab.Tagger(\"-O wakati\")\n",
    "    wakati.parse(\"\")\n",
    "    words = wakati.parse(text)\n",
    "\n",
    "    # Make word list\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [tokenize(a) for a in texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "\n",
    "\n",
    "from Ocab import Ocab, Regexp\n",
    "c = Regexp()\n",
    "m = Ocab(target=[\"名詞\",\"動詞\",\"形容詞\",\"副詞\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unknown_token = np.random.randint(-1, 1, (200, 1))  #\n",
    "unknown_token = np.random.random_sample(200)\n",
    "word2vec_array0 = []\n",
    "for str in texts:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array0.append(word_vectors)\n",
    "\n",
    "len(word2vec_array0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = 25\n",
    "input_data = []\n",
    "for vector in word2vec_array0:\n",
    "    context_vectors = vector\n",
    "    while(len(context_vectors)<input_length):\n",
    "        context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#     print(context_vectors)\n",
    "    input_data.append(context_vectors)\n",
    "len(input_data) #400\n",
    "len(input_data[0]) #25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data)\n",
    "# train_input_data, val_input_data, train_output_data, val_output_data = train_test_split(input_data,output_data,train_size=0.8, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "# print(input_data.shape)\n",
    "# print(output_data.shape)\n",
    "# print(len(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uchidayuki/Python/lib/python3.6/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 282 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "282/282 [==============================] - 10s 35ms/step - loss: 3.9081 - val_loss: 0.6097\n",
      "Epoch 2/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5903 - val_loss: 0.6331\n",
      "Epoch 3/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5688 - val_loss: 0.6275\n",
      "Epoch 4/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5399 - val_loss: 0.5653\n",
      "Epoch 5/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5087 - val_loss: 0.5940\n",
      "Epoch 6/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5605 - val_loss: 0.5836\n",
      "Epoch 7/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4976 - val_loss: 0.6413\n",
      "Epoch 8/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5732 - val_loss: 0.6707\n",
      "Epoch 9/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5876 - val_loss: 0.5784\n",
      "Epoch 10/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4921 - val_loss: 0.5769\n",
      "Epoch 11/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4923 - val_loss: 0.5673\n",
      "Epoch 12/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4954 - val_loss: 0.5960\n",
      "Epoch 13/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5668 - val_loss: 0.6163\n",
      "Epoch 14/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5876 - val_loss: 0.5709\n",
      "Epoch 15/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4944 - val_loss: 0.5793\n",
      "Epoch 16/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4925 - val_loss: 0.5937\n",
      "Epoch 17/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4992 - val_loss: 0.5694\n",
      "Epoch 18/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.4966 - val_loss: 0.6091\n",
      "Epoch 19/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.5783 - val_loss: 0.6075\n",
      "Epoch 20/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5131 - val_loss: 0.5650\n",
      "Epoch 21/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4969 - val_loss: 0.5768\n",
      "Epoch 22/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5477 - val_loss: 0.5724\n",
      "Epoch 23/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5416 - val_loss: 0.5615\n",
      "Epoch 24/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5265 - val_loss: 0.5911\n",
      "Epoch 25/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5731 - val_loss: 0.6208\n",
      "Epoch 26/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6075 - val_loss: 0.5688\n",
      "Epoch 27/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5419 - val_loss: 0.5782\n",
      "Epoch 28/100\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 0.5339 - val_loss: 0.5516\n",
      "Epoch 29/100\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 0.5050 - val_loss: 0.5966\n",
      "Epoch 30/100\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.5776 - val_loss: 0.5551\n",
      "Epoch 31/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.4442 - val_loss: 0.5498\n",
      "Epoch 32/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4556 - val_loss: 0.6536\n",
      "Epoch 33/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6820 - val_loss: 0.5830\n",
      "Epoch 34/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5620 - val_loss: 0.5283\n",
      "Epoch 35/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4401 - val_loss: 0.5297\n",
      "Epoch 36/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4313 - val_loss: 0.6095\n",
      "Epoch 37/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4931 - val_loss: 0.6258\n",
      "Epoch 38/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4957 - val_loss: 0.5390\n",
      "Epoch 39/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4160 - val_loss: 0.5460\n",
      "Epoch 40/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.5317 - val_loss: 0.6761\n",
      "Epoch 41/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.6853 - val_loss: 0.5212\n",
      "Epoch 42/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4365 - val_loss: 0.6096\n",
      "Epoch 43/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5645 - val_loss: 0.5218\n",
      "Epoch 44/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3739 - val_loss: 0.6642\n",
      "Epoch 45/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5084 - val_loss: 0.6075\n",
      "Epoch 46/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4350 - val_loss: 0.5212\n",
      "Epoch 47/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3625 - val_loss: 0.5651\n",
      "Epoch 48/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4598 - val_loss: 0.5868\n",
      "Epoch 49/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.6022 - val_loss: 0.5601\n",
      "Epoch 50/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5382 - val_loss: 0.4951\n",
      "Epoch 51/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3835 - val_loss: 0.4992\n",
      "Epoch 52/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4268 - val_loss: 0.4876\n",
      "Epoch 53/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4156 - val_loss: 0.4944\n",
      "Epoch 54/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3348 - val_loss: 0.5989\n",
      "Epoch 55/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4289 - val_loss: 0.6155\n",
      "Epoch 56/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4214 - val_loss: 0.5338\n",
      "Epoch 57/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3420 - val_loss: 0.5271\n",
      "Epoch 58/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3446 - val_loss: 0.6258\n",
      "Epoch 59/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.5408 - val_loss: 0.5202\n",
      "Epoch 60/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3506 - val_loss: 0.5103\n",
      "Epoch 61/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3347 - val_loss: 0.5250\n",
      "Epoch 62/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3836 - val_loss: 0.5555\n",
      "Epoch 63/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4275 - val_loss: 0.5978\n",
      "Epoch 64/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4390 - val_loss: 0.5270\n",
      "Epoch 65/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3095 - val_loss: 0.5550\n",
      "Epoch 66/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3418 - val_loss: 0.6761\n",
      "Epoch 67/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4821 - val_loss: 0.5750\n",
      "Epoch 68/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3456 - val_loss: 0.5217\n",
      "Epoch 69/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3153 - val_loss: 0.5504\n",
      "Epoch 70/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2929 - val_loss: 0.6412\n",
      "Epoch 71/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4198 - val_loss: 0.5994\n",
      "Epoch 72/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.3617 - val_loss: 0.5461\n",
      "Epoch 73/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.2975 - val_loss: 0.6739\n",
      "Epoch 74/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3917 - val_loss: 0.5643\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3036 - val_loss: 0.6671\n",
      "Epoch 76/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3875 - val_loss: 0.5500\n",
      "Epoch 77/100\n",
      "282/282 [==============================] - 2s 5ms/step - loss: 0.2807 - val_loss: 0.5261\n",
      "Epoch 78/100\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.2691 - val_loss: 0.5371\n",
      "Epoch 79/100\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.3993 - val_loss: 0.5559\n",
      "Epoch 80/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.4102 - val_loss: 0.5415\n",
      "Epoch 81/100\n",
      "282/282 [==============================] - 2s 8ms/step - loss: 0.3597 - val_loss: 0.5666\n",
      "Epoch 82/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3839 - val_loss: 0.5196\n",
      "Epoch 83/100\n",
      "282/282 [==============================] - 2s 7ms/step - loss: 0.3166 - val_loss: 0.5285\n",
      "Epoch 84/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3500 - val_loss: 0.5857\n",
      "Epoch 85/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3708 - val_loss: 0.5802\n",
      "Epoch 86/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3909 - val_loss: 0.5233\n",
      "Epoch 87/100\n",
      "282/282 [==============================] - 2s 6ms/step - loss: 0.3277 - val_loss: 0.5190\n",
      "Epoch 88/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2506 - val_loss: 0.5565\n",
      "Epoch 89/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2927 - val_loss: 0.5539\n",
      "Epoch 90/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2791 - val_loss: 0.6108\n",
      "Epoch 91/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3871 - val_loss: 0.5707\n",
      "Epoch 92/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3228 - val_loss: 0.6008\n",
      "Epoch 93/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.3472 - val_loss: 0.5269\n",
      "Epoch 94/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2522 - val_loss: 0.5274\n",
      "Epoch 95/100\n",
      "282/282 [==============================] - 1s 5ms/step - loss: 0.2304 - val_loss: 0.4951\n",
      "Epoch 96/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.2474 - val_loss: 0.5246\n",
      "Epoch 97/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3071 - val_loss: 0.6333\n",
      "Epoch 98/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.4451 - val_loss: 0.5111\n",
      "Epoch 99/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3104 - val_loss: 0.5299\n",
      "Epoch 100/100\n",
      "282/282 [==============================] - 1s 4ms/step - loss: 0.3157 - val_loss: 0.5169\n",
      "78/78 [==============================] - 0s 2ms/step\n",
      "0.5234231841869843\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_41 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 286 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "286/286 [==============================] - 8s 29ms/step - loss: 3.8158 - val_loss: 0.6245\n",
      "Epoch 2/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.6345 - val_loss: 0.5855\n",
      "Epoch 3/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5043 - val_loss: 0.5635\n",
      "Epoch 4/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5279 - val_loss: 0.6795\n",
      "Epoch 5/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.6457 - val_loss: 0.7065\n",
      "Epoch 6/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.6407 - val_loss: 0.5786\n",
      "Epoch 7/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5002 - val_loss: 0.6221\n",
      "Epoch 8/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.5578 - val_loss: 0.6022\n",
      "Epoch 9/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.5104 - val_loss: 0.5629\n",
      "Epoch 10/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.5175 - val_loss: 0.5868\n",
      "Epoch 11/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.5416 - val_loss: 0.5654\n",
      "Epoch 12/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.5084 - val_loss: 0.5639\n",
      "Epoch 13/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.5096 - val_loss: 0.5792\n",
      "Epoch 14/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4960 - val_loss: 0.5664\n",
      "Epoch 15/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.5071 - val_loss: 0.6037\n",
      "Epoch 16/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.5717 - val_loss: 0.5759\n",
      "Epoch 17/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4952 - val_loss: 0.5805\n",
      "Epoch 18/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4958 - val_loss: 0.5888\n",
      "Epoch 19/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.5043 - val_loss: 0.5719\n",
      "Epoch 20/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4937 - val_loss: 0.5656\n",
      "Epoch 21/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4915 - val_loss: 0.7147\n",
      "Epoch 22/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.7085 - val_loss: 0.6677\n",
      "Epoch 23/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.6011 - val_loss: 0.6107\n",
      "Epoch 24/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5222 - val_loss: 0.6234\n",
      "Epoch 25/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5444 - val_loss: 0.7035\n",
      "Epoch 26/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.6183 - val_loss: 0.5767\n",
      "Epoch 27/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4860 - val_loss: 0.5613\n",
      "Epoch 28/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4762 - val_loss: 0.5493\n",
      "Epoch 29/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4673 - val_loss: 0.5844\n",
      "Epoch 30/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5388 - val_loss: 0.5452\n",
      "Epoch 31/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4509 - val_loss: 0.6328\n",
      "Epoch 32/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5140 - val_loss: 0.5698\n",
      "Epoch 33/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4571 - val_loss: 0.5976\n",
      "Epoch 34/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4811 - val_loss: 0.5180\n",
      "Epoch 35/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4327 - val_loss: 0.7039\n",
      "Epoch 36/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.6695 - val_loss: 0.5159\n",
      "Epoch 37/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4405 - val_loss: 0.5171\n",
      "Epoch 38/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4260 - val_loss: 0.6057\n",
      "Epoch 39/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5548 - val_loss: 0.5675\n",
      "Epoch 40/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.5128 - val_loss: 0.5106\n",
      "Epoch 41/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4077 - val_loss: 0.5381\n",
      "Epoch 42/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4703 - val_loss: 0.4932\n",
      "Epoch 43/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3685 - val_loss: 0.5176\n",
      "Epoch 44/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3867 - val_loss: 0.5423\n",
      "Epoch 45/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4769 - val_loss: 0.4938\n",
      "Epoch 46/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3561 - val_loss: 0.5248\n",
      "Epoch 47/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4763 - val_loss: 0.5392\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4971 - val_loss: 0.4820\n",
      "Epoch 49/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3769 - val_loss: 0.5134\n",
      "Epoch 50/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3821 - val_loss: 0.5042\n",
      "Epoch 51/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4402 - val_loss: 0.5034\n",
      "Epoch 52/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3856 - val_loss: 0.4946\n",
      "Epoch 53/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.4004 - val_loss: 0.4820\n",
      "Epoch 54/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3658 - val_loss: 0.4877\n",
      "Epoch 55/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.3098 - val_loss: 0.5566\n",
      "Epoch 56/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4385 - val_loss: 0.4916\n",
      "Epoch 57/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3984 - val_loss: 0.4930\n",
      "Epoch 58/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3365 - val_loss: 0.4900\n",
      "Epoch 59/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.3074 - val_loss: 0.5185\n",
      "Epoch 60/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4520 - val_loss: 0.5261\n",
      "Epoch 61/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3777 - val_loss: 0.5067\n",
      "Epoch 62/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.3011 - val_loss: 0.5487\n",
      "Epoch 63/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.3838 - val_loss: 0.5220\n",
      "Epoch 64/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3533 - val_loss: 0.4955\n",
      "Epoch 65/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3108 - val_loss: 0.5167\n",
      "Epoch 66/100\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.3722 - val_loss: 0.4917\n",
      "Epoch 67/100\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 0.3469 - val_loss: 0.5515\n",
      "Epoch 68/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.4267 - val_loss: 0.4869\n",
      "Epoch 69/100\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 0.3535 - val_loss: 0.4817\n",
      "Epoch 70/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.2783 - val_loss: 0.4936\n",
      "Epoch 71/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2596 - val_loss: 0.4729\n",
      "Epoch 72/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2810 - val_loss: 0.4823\n",
      "Epoch 73/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3667 - val_loss: 0.4727\n",
      "Epoch 74/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3496 - val_loss: 0.4501\n",
      "Epoch 75/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2881 - val_loss: 0.4637\n",
      "Epoch 76/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3294 - val_loss: 0.4537\n",
      "Epoch 77/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2816 - val_loss: 0.4734\n",
      "Epoch 78/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2616 - val_loss: 0.4489\n",
      "Epoch 79/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2509 - val_loss: 0.4944\n",
      "Epoch 80/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3936 - val_loss: 0.4864\n",
      "Epoch 81/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3575 - val_loss: 0.4894\n",
      "Epoch 82/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3657 - val_loss: 0.4639\n",
      "Epoch 83/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.3012 - val_loss: 0.4558\n",
      "Epoch 84/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2779 - val_loss: 0.4825\n",
      "Epoch 85/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2954 - val_loss: 0.4686\n",
      "Epoch 86/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3059 - val_loss: 0.4605\n",
      "Epoch 87/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2497 - val_loss: 0.4765\n",
      "Epoch 88/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2294 - val_loss: 0.5183\n",
      "Epoch 89/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2597 - val_loss: 0.5708\n",
      "Epoch 90/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3201 - val_loss: 0.4910\n",
      "Epoch 91/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2285 - val_loss: 0.5399\n",
      "Epoch 92/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3056 - val_loss: 0.5629\n",
      "Epoch 93/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.3374 - val_loss: 0.5337\n",
      "Epoch 94/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2924 - val_loss: 0.5237\n",
      "Epoch 95/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2773 - val_loss: 0.4817\n",
      "Epoch 96/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2226 - val_loss: 0.4677\n",
      "Epoch 97/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2063 - val_loss: 0.4458\n",
      "Epoch 98/100\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 0.2283 - val_loss: 0.4898\n",
      "Epoch 99/100\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 0.3267 - val_loss: 0.4985\n",
      "Epoch 100/100\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 0.3339 - val_loss: 0.4834\n",
      "74/74 [==============================] - 0s 3ms/step\n",
      "0.48426922108675985\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 288 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "288/288 [==============================] - 20s 70ms/step - loss: 3.7814 - val_loss: 0.8659\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.8364 - val_loss: 0.5651\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5109 - val_loss: 0.5744\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.4990 - val_loss: 0.5694\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.5026 - val_loss: 0.5688\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - 3s 9ms/step - loss: 0.5056 - val_loss: 0.6234\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.5876 - val_loss: 0.6405\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.6129 - val_loss: 0.5645\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.5130 - val_loss: 0.5715\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4999 - val_loss: 0.7012\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.6315 - val_loss: 0.5840\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5034 - val_loss: 0.5725\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5025 - val_loss: 0.9752\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.8782 - val_loss: 0.5884\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - 2s 9ms/step - loss: 0.5078 - val_loss: 0.5760\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4986 - val_loss: 0.5732\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4994 - val_loss: 0.6942\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.6193 - val_loss: 0.5685\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.5023 - val_loss: 0.5748\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5029 - val_loss: 0.5822\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5450 - val_loss: 0.5907\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5029 - val_loss: 0.5898\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - 2s 9ms/step - loss: 0.5557 - val_loss: 0.5829\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5028 - val_loss: 0.5775\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4986 - val_loss: 0.5756\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4980 - val_loss: 0.6013\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5309 - val_loss: 0.6234\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5518 - val_loss: 0.6612\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5885 - val_loss: 0.6106\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.5375 - val_loss: 0.5846\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5075 - val_loss: 0.5850\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5065 - val_loss: 0.5860\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5021 - val_loss: 0.5961\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5717 - val_loss: 0.6425\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.6217 - val_loss: 0.5938\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5691 - val_loss: 0.6146\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5837 - val_loss: 0.5670\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5389 - val_loss: 0.6182\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5793 - val_loss: 0.6028\n",
      "Epoch 40/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5239 - val_loss: 0.5860\n",
      "Epoch 41/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5142 - val_loss: 0.6503\n",
      "Epoch 42/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5647 - val_loss: 0.5819\n",
      "Epoch 43/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4997 - val_loss: 0.5674\n",
      "Epoch 44/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4916 - val_loss: 0.7202\n",
      "Epoch 45/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.6651 - val_loss: 0.5784\n",
      "Epoch 46/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4926 - val_loss: 0.5667\n",
      "Epoch 47/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.4866 - val_loss: 0.6532\n",
      "Epoch 48/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5720 - val_loss: 0.5585\n",
      "Epoch 49/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.4984 - val_loss: 0.5775\n",
      "Epoch 50/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5267 - val_loss: 0.5601\n",
      "Epoch 51/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5253 - val_loss: 0.6633\n",
      "Epoch 52/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.6398 - val_loss: 0.5506\n",
      "Epoch 53/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4837 - val_loss: 0.5453\n",
      "Epoch 54/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4696 - val_loss: 0.9779\n",
      "Epoch 55/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.9218 - val_loss: 0.5913\n",
      "Epoch 56/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.5522 - val_loss: 0.5594\n",
      "Epoch 57/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4565 - val_loss: 0.5685\n",
      "Epoch 58/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4660 - val_loss: 0.6382\n",
      "Epoch 59/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5389 - val_loss: 0.5595\n",
      "Epoch 60/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4324 - val_loss: 0.5434\n",
      "Epoch 61/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4097 - val_loss: 0.5674\n",
      "Epoch 62/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.4330 - val_loss: 0.6002\n",
      "Epoch 63/100\n",
      "288/288 [==============================] - 3s 9ms/step - loss: 0.4696 - val_loss: 0.6363\n",
      "Epoch 64/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5060 - val_loss: 0.5652\n",
      "Epoch 65/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4081 - val_loss: 0.6411\n",
      "Epoch 66/100\n",
      "288/288 [==============================] - 2s 9ms/step - loss: 0.4855 - val_loss: 0.5155\n",
      "Epoch 67/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.4160 - val_loss: 0.5251\n",
      "Epoch 68/100\n",
      "288/288 [==============================] - 3s 9ms/step - loss: 0.4787 - val_loss: 0.5360\n",
      "Epoch 69/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4986 - val_loss: 0.5551\n",
      "Epoch 70/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4983 - val_loss: 0.5274\n",
      "Epoch 71/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3600 - val_loss: 0.5774\n",
      "Epoch 72/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3991 - val_loss: 0.5348\n",
      "Epoch 73/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3490 - val_loss: 0.4846\n",
      "Epoch 74/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3583 - val_loss: 0.7134\n",
      "Epoch 75/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.7256 - val_loss: 0.5091\n",
      "Epoch 76/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.4663 - val_loss: 0.5130\n",
      "Epoch 77/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3256 - val_loss: 0.4972\n",
      "Epoch 78/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4006 - val_loss: 0.5287\n",
      "Epoch 79/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5133 - val_loss: 0.4880\n",
      "Epoch 80/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4602 - val_loss: 0.4692\n",
      "Epoch 81/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3495 - val_loss: 0.4776\n",
      "Epoch 82/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4557\n",
      "Epoch 83/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3847 - val_loss: 0.4862\n",
      "Epoch 84/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4269 - val_loss: 0.4443\n",
      "Epoch 85/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3605 - val_loss: 0.4539\n",
      "Epoch 86/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3963 - val_loss: 0.4652\n",
      "Epoch 87/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.2943 - val_loss: 0.4985\n",
      "Epoch 88/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.2740 - val_loss: 0.4900\n",
      "Epoch 89/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.2704 - val_loss: 0.6467\n",
      "Epoch 90/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4856 - val_loss: 0.6223\n",
      "Epoch 91/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4364 - val_loss: 0.5718\n",
      "Epoch 92/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3409 - val_loss: 0.5247\n",
      "Epoch 93/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3375 - val_loss: 0.5685\n",
      "Epoch 94/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.3964 - val_loss: 0.5387\n",
      "Epoch 95/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3392 - val_loss: 0.5643\n",
      "Epoch 96/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3496 - val_loss: 0.5807\n",
      "Epoch 97/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3628 - val_loss: 0.5431\n",
      "Epoch 98/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.3195 - val_loss: 0.5234\n",
      "Epoch 99/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.2809 - val_loss: 0.4971\n",
      "Epoch 100/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.2839 - val_loss: 0.6049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 0s 3ms/step\n",
      "0.5944781833224826\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_43 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 288 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "288/288 [==============================] - 9s 32ms/step - loss: 3.8022 - val_loss: 0.8681\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.8868 - val_loss: 0.5634\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5099 - val_loss: 0.6458\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5522 - val_loss: 0.5866\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5420 - val_loss: 0.6057\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5281 - val_loss: 0.6177\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5270 - val_loss: 0.5648\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5076 - val_loss: 0.5772\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4943 - val_loss: 0.5986\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5236 - val_loss: 0.6454\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.5768 - val_loss: 0.6785\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.5998 - val_loss: 0.5784\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4951 - val_loss: 0.5870\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5029 - val_loss: 0.6204\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5383 - val_loss: 0.5744\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4952 - val_loss: 0.5632\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5289 - val_loss: 0.5867\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5513 - val_loss: 0.5644\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5094 - val_loss: 0.5767\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5348 - val_loss: 0.5630\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4993 - val_loss: 0.5597\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4973 - val_loss: 0.6378\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5484 - val_loss: 0.5552\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4920 - val_loss: 0.5542\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.5023 - val_loss: 0.6094\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5576 - val_loss: 0.7142\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.6387 - val_loss: 0.5657\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4780 - val_loss: 0.6162\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5234 - val_loss: 0.5924\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4884 - val_loss: 0.5737\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4731 - val_loss: 0.6096\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5166 - val_loss: 0.6446\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5231 - val_loss: 0.5490\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4630 - val_loss: 0.6089\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4956 - val_loss: 0.5759\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4526 - val_loss: 0.6039\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4850 - val_loss: 0.5844\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4820 - val_loss: 0.6652\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.5487 - val_loss: 0.5529\n",
      "Epoch 40/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.4438 - val_loss: 0.5405\n",
      "Epoch 41/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4442 - val_loss: 0.5328\n",
      "Epoch 42/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.4906 - val_loss: 0.6191\n",
      "Epoch 43/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5835 - val_loss: 0.5472\n",
      "Epoch 44/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4389 - val_loss: 0.5776\n",
      "Epoch 45/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5337 - val_loss: 0.5702\n",
      "Epoch 46/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5016 - val_loss: 0.5446\n",
      "Epoch 47/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4322 - val_loss: 0.5565\n",
      "Epoch 48/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4883 - val_loss: 0.5407\n",
      "Epoch 49/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4782 - val_loss: 0.5977\n",
      "Epoch 50/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5330 - val_loss: 0.5390\n",
      "Epoch 51/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.5385\n",
      "Epoch 52/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4228 - val_loss: 0.5711\n",
      "Epoch 53/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5357 - val_loss: 0.5934\n",
      "Epoch 54/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5240 - val_loss: 0.5112\n",
      "Epoch 55/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3835 - val_loss: 0.5173\n",
      "Epoch 56/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4070 - val_loss: 0.5422\n",
      "Epoch 57/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3692 - val_loss: 0.5144\n",
      "Epoch 58/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4424 - val_loss: 0.5815\n",
      "Epoch 59/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.5176 - val_loss: 0.5446\n",
      "Epoch 60/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4520 - val_loss: 0.5065\n",
      "Epoch 61/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3676 - val_loss: 0.5040\n",
      "Epoch 62/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3454 - val_loss: 0.5217\n",
      "Epoch 63/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4173 - val_loss: 0.5271\n",
      "Epoch 64/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4722 - val_loss: 0.5160\n",
      "Epoch 65/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4207 - val_loss: 0.5596\n",
      "Epoch 66/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.4513 - val_loss: 0.5143\n",
      "Epoch 67/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3209 - val_loss: 0.5083\n",
      "Epoch 68/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3662 - val_loss: 0.5383\n",
      "Epoch 69/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.4810 - val_loss: 0.4980\n",
      "Epoch 70/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3494 - val_loss: 0.5113\n",
      "Epoch 71/100\n",
      "288/288 [==============================] - 3s 10ms/step - loss: 0.4234 - val_loss: 0.4783\n",
      "Epoch 72/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.3286 - val_loss: 0.4610\n",
      "Epoch 73/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.3535 - val_loss: 0.5070\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4047 - val_loss: 0.5132\n",
      "Epoch 75/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3464 - val_loss: 0.5076\n",
      "Epoch 76/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3912 - val_loss: 0.4976\n",
      "Epoch 77/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3023 - val_loss: 0.5350\n",
      "Epoch 78/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.3730 - val_loss: 0.5245\n",
      "Epoch 79/100\n",
      "288/288 [==============================] - 2s 8ms/step - loss: 0.4187 - val_loss: 0.5041\n",
      "Epoch 80/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.3210 - val_loss: 0.5330\n",
      "Epoch 81/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.3075 - val_loss: 0.5081\n",
      "Epoch 82/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3382 - val_loss: 0.5222\n",
      "Epoch 83/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.4252 - val_loss: 0.4857\n",
      "Epoch 84/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3347 - val_loss: 0.5222\n",
      "Epoch 85/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3654 - val_loss: 0.5008\n",
      "Epoch 86/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3638 - val_loss: 0.5343\n",
      "Epoch 87/100\n",
      "288/288 [==============================] - 2s 5ms/step - loss: 0.3726 - val_loss: 0.4939\n",
      "Epoch 88/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3036 - val_loss: 0.5416\n",
      "Epoch 89/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3634 - val_loss: 0.5438\n",
      "Epoch 90/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3269 - val_loss: 0.5678\n",
      "Epoch 91/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.3517 - val_loss: 0.4958\n",
      "Epoch 92/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.2661 - val_loss: 0.5277\n",
      "Epoch 93/100\n",
      "288/288 [==============================] - 1s 4ms/step - loss: 0.2666 - val_loss: 0.6059\n",
      "Epoch 94/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3785 - val_loss: 0.5557\n",
      "Epoch 95/100\n",
      "288/288 [==============================] - 2s 7ms/step - loss: 0.3031 - val_loss: 0.5750\n",
      "Epoch 96/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3396 - val_loss: 0.5825\n",
      "Epoch 97/100\n",
      "288/288 [==============================] - 2s 6ms/step - loss: 0.3481 - val_loss: 0.5593\n",
      "Epoch 98/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3072 - val_loss: 0.6020\n",
      "Epoch 99/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.3153 - val_loss: 0.5447\n",
      "Epoch 100/100\n",
      "288/288 [==============================] - 1s 5ms/step - loss: 0.2445 - val_loss: 0.5020\n",
      "72/72 [==============================] - 0s 3ms/step\n",
      "0.46878820326593185\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_44 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 296 samples, validate on 40 samples\n",
      "Epoch 1/100\n",
      "296/296 [==============================] - 13s 44ms/step - loss: 3.8228 - val_loss: 0.6489\n",
      "Epoch 2/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.6722 - val_loss: 0.5831\n",
      "Epoch 3/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4965 - val_loss: 0.5965\n",
      "Epoch 4/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5845 - val_loss: 0.7412\n",
      "Epoch 5/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.7317 - val_loss: 0.5808\n",
      "Epoch 6/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4970 - val_loss: 0.5683\n",
      "Epoch 7/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4978 - val_loss: 0.6938\n",
      "Epoch 8/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.6168 - val_loss: 0.6023\n",
      "Epoch 9/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5473 - val_loss: 0.8678\n",
      "Epoch 10/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.7684 - val_loss: 0.5750\n",
      "Epoch 11/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4946 - val_loss: 0.5685\n",
      "Epoch 12/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4973 - val_loss: 0.5791\n",
      "Epoch 13/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.4938 - val_loss: 0.5650\n",
      "Epoch 14/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5147 - val_loss: 0.6149\n",
      "Epoch 15/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5823 - val_loss: 0.5716\n",
      "Epoch 16/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.5293 - val_loss: 0.6241\n",
      "Epoch 17/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.5758 - val_loss: 0.7083\n",
      "Epoch 18/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.6264 - val_loss: 0.5865\n",
      "Epoch 19/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4990 - val_loss: 0.6777\n",
      "Epoch 20/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.6667 - val_loss: 0.5662\n",
      "Epoch 21/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5104 - val_loss: 0.5736\n",
      "Epoch 22/100\n",
      "296/296 [==============================] - 3s 9ms/step - loss: 0.4989 - val_loss: 0.6053\n",
      "Epoch 23/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.5798 - val_loss: 0.5752\n",
      "Epoch 24/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5287 - val_loss: 0.5865\n",
      "Epoch 25/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5009 - val_loss: 0.6029\n",
      "Epoch 26/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5305 - val_loss: 0.6604\n",
      "Epoch 27/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5865 - val_loss: 0.5860\n",
      "Epoch 28/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5039 - val_loss: 0.7239\n",
      "Epoch 29/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.6315 - val_loss: 0.5834\n",
      "Epoch 30/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5599 - val_loss: 0.6067\n",
      "Epoch 31/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.5849 - val_loss: 0.6465\n",
      "Epoch 32/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.5499 - val_loss: 0.5631\n",
      "Epoch 33/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5077 - val_loss: 0.5913\n",
      "Epoch 34/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.5618 - val_loss: 0.6321\n",
      "Epoch 35/100\n",
      "296/296 [==============================] - 3s 9ms/step - loss: 0.6108 - val_loss: 0.5623\n",
      "Epoch 36/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.5294 - val_loss: 0.5796\n",
      "Epoch 37/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.5425 - val_loss: 0.5586\n",
      "Epoch 38/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5126 - val_loss: 0.5565\n",
      "Epoch 39/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5203 - val_loss: 0.5960\n",
      "Epoch 40/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5502 - val_loss: 0.5639\n",
      "Epoch 41/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.4652 - val_loss: 0.5530\n",
      "Epoch 42/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.4523 - val_loss: 0.8787\n",
      "Epoch 43/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.7891 - val_loss: 0.5759\n",
      "Epoch 44/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.4810 - val_loss: 0.6493\n",
      "Epoch 45/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.5126 - val_loss: 0.5380\n",
      "Epoch 46/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.4485 - val_loss: 0.6567\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/296 [==============================] - 2s 8ms/step - loss: 0.6489 - val_loss: 0.5477\n",
      "Epoch 48/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.4853 - val_loss: 0.5679\n",
      "Epoch 49/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.4403 - val_loss: 0.6580\n",
      "Epoch 50/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.5205 - val_loss: 0.5682\n",
      "Epoch 51/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4231 - val_loss: 0.5850\n",
      "Epoch 52/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4540 - val_loss: 0.6730\n",
      "Epoch 53/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5345 - val_loss: 0.5811\n",
      "Epoch 54/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4307 - val_loss: 0.5690\n",
      "Epoch 55/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.4376 - val_loss: 0.6540\n",
      "Epoch 56/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5056 - val_loss: 0.6279\n",
      "Epoch 57/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5110 - val_loss: 0.5751\n",
      "Epoch 58/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4396 - val_loss: 0.5325\n",
      "Epoch 59/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3845 - val_loss: 0.5012\n",
      "Epoch 60/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3747 - val_loss: 0.6754\n",
      "Epoch 61/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.5229 - val_loss: 0.5383\n",
      "Epoch 62/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.3675 - val_loss: 0.4990\n",
      "Epoch 63/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3617 - val_loss: 0.5491\n",
      "Epoch 64/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.5124 - val_loss: 0.5804\n",
      "Epoch 65/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.5226 - val_loss: 0.4945\n",
      "Epoch 66/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.4037 - val_loss: 0.6584\n",
      "Epoch 67/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.6327 - val_loss: 0.4792\n",
      "Epoch 68/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.4347 - val_loss: 0.5150\n",
      "Epoch 69/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.4707 - val_loss: 0.4855\n",
      "Epoch 70/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.3390 - val_loss: 0.5390\n",
      "Epoch 71/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.4912 - val_loss: 0.5073\n",
      "Epoch 72/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.4156 - val_loss: 0.5611\n",
      "Epoch 73/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.5105 - val_loss: 0.4795\n",
      "Epoch 74/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.4278 - val_loss: 0.4636\n",
      "Epoch 75/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.3282 - val_loss: 0.4633\n",
      "Epoch 76/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.3304 - val_loss: 0.4576\n",
      "Epoch 77/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.3759 - val_loss: 0.4570\n",
      "Epoch 78/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.3925 - val_loss: 0.5406\n",
      "Epoch 79/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.4848 - val_loss: 0.4556\n",
      "Epoch 80/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.3097 - val_loss: 0.4987\n",
      "Epoch 81/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.3335 - val_loss: 0.6127\n",
      "Epoch 82/100\n",
      "296/296 [==============================] - 2s 7ms/step - loss: 0.5266 - val_loss: 0.5542\n",
      "Epoch 83/100\n",
      "296/296 [==============================] - 2s 8ms/step - loss: 0.3759 - val_loss: 0.5688\n",
      "Epoch 84/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.3719 - val_loss: 0.5427\n",
      "Epoch 85/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3501 - val_loss: 0.5411\n",
      "Epoch 86/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3297 - val_loss: 0.5697\n",
      "Epoch 87/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3695 - val_loss: 0.5457\n",
      "Epoch 88/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3320 - val_loss: 0.5896\n",
      "Epoch 89/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3786 - val_loss: 0.5289\n",
      "Epoch 90/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.3029 - val_loss: 0.5144\n",
      "Epoch 91/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.2929 - val_loss: 0.5796\n",
      "Epoch 92/100\n",
      "296/296 [==============================] - 1s 4ms/step - loss: 0.3616 - val_loss: 0.4922\n",
      "Epoch 93/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.2665 - val_loss: 0.4964\n",
      "Epoch 94/100\n",
      "296/296 [==============================] - 2s 6ms/step - loss: 0.2644 - val_loss: 0.6032\n",
      "Epoch 95/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.4219 - val_loss: 0.5616\n",
      "Epoch 96/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.3286 - val_loss: 0.5322\n",
      "Epoch 97/100\n",
      "296/296 [==============================] - 1s 5ms/step - loss: 0.3072 - val_loss: 0.6957\n",
      "Epoch 98/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.4549 - val_loss: 0.5651\n",
      "Epoch 99/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.3349 - val_loss: 0.5052\n",
      "Epoch 100/100\n",
      "296/296 [==============================] - 2s 5ms/step - loss: 0.2599 - val_loss: 0.5850\n",
      "64/64 [==============================] - 0s 2ms/step\n",
      "0.5575863420963287\n",
      "[0.5234231841869843, 0.48426922108675985, 0.5944781833224826, 0.46878820326593185, 0.5575863420963287]\n",
      "0.5257090267916975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras import backend as K \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean, median,variance,stdev\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_train = int(len(input_data) * 0.9)\n",
    "N_validation = len(input_data) - N_train\n",
    "\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation, row_train, row_validation = \\\n",
    "    train_test_split(input_data, labels, texts, test_size=N_validation)\n",
    "\n",
    "\n",
    "maxlen = 25\n",
    "'''\n",
    "モデル設定\n",
    "'''\n",
    "n_in = 200  \n",
    "n_hidden = 512\n",
    "n_out = 1\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred): \n",
    "     return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fold_num = 5 \n",
    "seed = 5\n",
    "np.random.seed(seed)\n",
    "kfold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_train, Y_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden,\n",
    "                   kernel_initializer=weight_variable,\n",
    "                   input_shape=(maxlen, n_in)))\n",
    "    # model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    # model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "\n",
    "\n",
    "    model.summary()\n",
    "    # optimizer = Adam(lr=0.0010, beta_1=0.9, beta_2=0.999)\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    # model.compile(loss='mean_squared_error',optimizer=optimizer)\n",
    "    model.compile(loss=root_mean_squared_error,\n",
    "                  optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    モデル学習\n",
    "    '''\n",
    "    epochs = 100\n",
    "    batch_size = 256\n",
    "\n",
    "\n",
    "    #earlystoppingなし\n",
    "#     model.fit(X_train[train], Y_train[test],\n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs,\n",
    "#               validation_data=(X_validation, Y_validation))\n",
    "    model.fit(X_train[train], Y_train[train],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(X_validation, Y_validation))\n",
    "\n",
    "    scores = model.evaluate(X_train[test], Y_train[test])\n",
    "    print(scores)\n",
    "    cvscores.append(scores)\n",
    "\n",
    "print(cvscores)\n",
    "print(mean(cvscores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch 32 epochs 100    mean   0.47\n",
    "#### batch 64 epochs 100    mean   0.50\n",
    "#### batch 128 epochs 100 mean 0.53  [0.5520341442181513, 0.5536023104513014, 0.5204473402765062, 0.45064101616541546, 0.55865877866745]\n",
    "#### batch 256 epochs 100 mean \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今回 は 品川 ブリンス 2 回 目 です が とても 満足 し て い ます  [3.2231724] 4.7\n",
      "夕食 、 朝食 共 に 種類 が 豊富 で 、 美味しく 食べ すぎ て しまい まし た  [4.363337] 4.5\n",
      "また 、 足湯 が あり 、 そこ から は 芦ノ湖 の 景色 が とても きれい に 見え まし た  [4.3757734] 4.5\n",
      "お 部屋 は ダブル ベット を くっつけ て 利用 でき た ので 、 子ども が 落ちる かも 、 など の 心配 を せ ず ぐっすり 休め まし た  [4.3734064] 4.7\n",
      "立地 が よく 、 電車 で 降り て から 荷物 を 預け て ユニバ に 行け た の が とても 便利 でし た  [4.4440517] 4.4\n",
      "この 料金 で ここ まで の サービス を 受け られる の は お 得 だ と 思い ます  [3.3677924] 3.4\n",
      "子供 が 小さい ので 、 早め に チェックイン し て 、 休憩  [3.5640833] 4.0\n",
      "卓球 ・ カラオケ ・ 麻雀 ・ 囲碁 ・ ・ ・ 全て 無料 と は 驚き です  [3.0730922] 3.2\n",
      "今回 は 忘れ物 を し て しまい 、 丁寧 に 取り計らっ て いただき まし た  [4.3757668] 4.5\n",
      "お 風呂 は 窓 が あり 海 が 見え て 、 洗面 を 挟ん で シャワー 室 が あり 素敵 でし た  [4.0511274] 4.4\n",
      "お 値段 は 少し 高く て も ハナ 館 で 宿泊 する 事 を 強く オススメ し ます  [4.0743694] 3.1\n",
      "お 掃除 スタッフ の 方 が 海外 から の お客様 に 丁寧 に 対応 さ れ て い た の を み て 、 素敵 だ と 思い まし た  [4.3475437] 4.2\n",
      "空調 等 設備 は 古い です が 、 問題 は なかっ た と 思い ます  [3.331078] 4.1\n",
      "また 館内 用 の 浴衣 の デザイン も 素敵 で オシャレ だ な と 思い まし た  [4.068351] 4.0\n",
      "今回 は 一 泊 二 日 だっ た ので 、 次回 は 二 泊 くらい し て 、 ゆっくり 滞在 し たい です  [3.9789798] 4.0\n",
      "開放 的 な 三角 の 屋根 の 天井 で 、 ログハウス を 満喫 でき ます 。  [2.8689497] 2.8\n",
      "私 が こんなに リピーター に なっ て いる の は 、 なんと いっ て も 蟹 です  [4.2381835] 4.6\n",
      "リニューアル さ れ た 館内 は とても 清潔 感 が あり 、 朝食 も 簡単 な ブッフェ でし た が 十分 でし た  [4.391785] 4.9\n",
      "ホテル まで は 駅 から も 徒歩 で ５ 分 くらい 行ける ので 便利 です  [3.129266] 3.2\n",
      "今後 は ディズニー に 行く なら この ホテル か 、 ミラコスタ 、 どちら か に しよ う と 思い ます ！  [3.4529288] 3.2\n",
      "小さい 子供 が いる ので 洗い場 付き の お 風呂 や 貸し出し の 踏み台 等 助かり まし た  [4.407694] 4.8\n",
      "な 大人 平日 一 泊 二 食 で ９ 千 円 くらい です  [2.8317804] 2.6\n",
      "アネックスタワー と メイン タワー で 全部 楽しめ ます  [3.2112126] 3.7\n",
      "朝 も 利用 し まし た が 、 美顔 器 を 朝 から 使え て 嬉しかっ た です  [3.8626108] 4.6\n",
      "お 部屋 も 快適 で 、 何より 窓 から 花火 が 綺麗 に 見え た のに 感動 し まし た  [4.37728] 4.8\n",
      "お 肉 と 魚 は 出来 立て を サーブ し て もらえ 、 デザート も 種類 は 多く 満腹 に なる まで 楽しめ まし た  [4.3857727] 4.4\n",
      "アクアビート は 天気 を 気 に せ ず 楽しめ て 、 5 才 の 娘 も 大 喜び でし た  [4.3777833] 4.1\n",
      "どこ に どんな スーパー が あり 、 銭湯 が あり 、 レストラン が ある の か 、 という 地図 も 分かり やすい です  [4.1515803] 3.0\n",
      "施設 の 老朽 化 は 否め ない が 、 金額 に対して 施設 、 サービス 、 食事 、 総て に 於い て 満足 です  [3.7846344] 4.8\n",
      "アクセス が 良い ので 仕事 ギリギリ まで ゆっくり できる の も 良い です ね  [4.366475] 3.1\n",
      "落ち着い た 大人 の 朝食 時間 を 過ごす に は いい の か な 、 と 感じ まし た  [4.2540317] 3.8\n",
      "チェックイン 時 に 部屋 を ジュニアスイート に グレード アップ さ せ て 頂き まし た … と 部屋 に 入る と 確か に 綺麗 で 広い お 部屋 でし た  [4.338447] 4.1\n",
      "次男 は 90 分間 もくもくと 大人しく 食べ 続け 、 おかげ で 大人 も ゆっくり 食事 を 満喫 でき まし た  [4.4868536] 4.7\n",
      "朝食 も バイキング を 頂き まし た が 、 子供 用 に カレー を 甘く し て 頂け たり 気づかい が 素晴らしかっ た です  [4.423333] 4.8\n",
      "ログハウス は 全 棟 リビング に 薪 ストーブ が 設置 し て あり ます 。  [2.022905] 2.8\n",
      "館内 の 連絡 通路 が 工事 中 で 通れ ませ ん でし た が 、 シャトル バス が 次々 に 来る ので 全然 不便 で は あり ませ ん でし た  [3.7557743] 4.7\n",
      "新潟 方面 を 回っ て やってき て 用向き は 渋谷 、 帰宅 は 羽田 だっ た ので 地理 的 に 品川 が 便利 で 決め て い ます  [3.0056016] 3.1\n",
      "朝食 ＆ 駐車 場 無料 の プラン で 3 歳 の 息子 を 連れ 家族 3 人 で お 邪魔 し まし た  [4.362796] 4.4\n",
      "朝食 も 良かっ た です よ （ ハワイ の そこ と 比べ て しまう と ちょっと 負け ます が ）  [4.0101976] 4.1\n",
      "夏 に 私 たち 夫婦 にとって 大変 つらい こと が あり 、 元気 に なる ため の 旅行 で も あり まし た  [4.0339007] 3.7\n"
     ]
    }
   ],
   "source": [
    "# test_vectors = np.array(test_vectors)\n",
    "y_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_validation)\n",
    "# for text, pred in zip(row_train, y_train_pred):\n",
    "#     print(text, pred)\n",
    "\n",
    "for text, pred, ans in zip(row_validation, y_test_pred, Y_validation):\n",
    "    print(text, pred, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早く死んでくれ [3.8762076]\n",
      "バカだなぁ [3.2928483]\n",
      "昨日、バイトがあって、お客さんがたくさん来ました [4.103437]\n",
      "浅井さんは丁寧で、非常に優しい [3.8676064]\n",
      "前にここに来たときは、大雪だったので、あまり楽しめませんでした [4.1435385]\n",
      "宿は長野県にある [2.6897705]\n",
      "この旅館は人気がある [3.7350123]\n",
      "従業員の方々は非常に優しく、対応が丁寧です [4.15258]\n",
      "従業員の方々が優しかったです [3.3799684]\n",
      "佐々木希が来たらしい [3.4493246]\n",
      "料理がとっても美味しくて、最高でした！ [4.270725]\n",
      "非常に景色と温泉が綺麗で、部屋の内装やアメニティが良く、非常に満足出来るものでした [4.1179886]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "出力を用いて予測\n",
    "'''\n",
    "# 元データの最初の一部だけ切り出し\n",
    "# model.predict()\n",
    "# input_data[3:10].shape\n",
    "\n",
    "test_text = [ \"早く死んでくれ\",\"バカだなぁ\",\"昨日、バイトがあって、お客さんがたくさん来ました\",\"浅井さんは丁寧で、非常に優しい\",\"前にここに来たときは、大雪だったので、あまり楽しめませんでした\",\"宿は長野県にある\",\"この旅館は人気がある\",\"従業員の方々は非常に優しく、対応が丁寧です\",\"従業員の方々が優しかったです\",\"佐々木希が来たらしい\",\"料理がとっても美味しくて、最高でした！\",\"非常に景色と温泉が綺麗で、部屋の内装やアメニティが良く、非常に満足出来るものでした\"]\n",
    "test_wakati_text = []\n",
    "for text in test_text:\n",
    "    test_wakati_text.append(tokenize(text))\n",
    "\n",
    "# test_wakati_text = \"\"\n",
    "word_model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "test_vectors = []\n",
    "for wakati_text in test_wakati_text:\n",
    "    test_vector = []\n",
    "    for char in m.removeStoplist(wakati_text, []).split(\" \"):\n",
    "        try:\n",
    "            test_vector.append(word_model[char])\n",
    "        except KeyError:\n",
    "            test_vector.append(unknown_token)\n",
    "    test_vectors.append(np.array(test_vector))\n",
    "\n",
    "# test_vectors = np.array(test_vectors)\n",
    "input_length = 25\n",
    "input_test_data = []\n",
    "for test_vector in test_vectors:\n",
    "    context_vectors = test_vector\n",
    "    while(len(context_vectors)<input_length):\n",
    "        context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "    input_test_data.append(context_vectors)\n",
    "input_test_data = np.array(input_test_data)\n",
    "\n",
    "\n",
    "test_predictions = model.predict(input_test_data)\n",
    "\n",
    "for pred, text in zip(test_predictions, test_text):\n",
    "    print(text, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
