{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_data = pd.read_csv(\"./some.csv\",encoding='cp932',header=None).drop(0,axis=0).drop(1,axis=1).drop(0,axis=1)\n",
    "columns0 = np.insert(df_data.iloc[0].values,0,\"keyword\") \n",
    "columns1 = np.insert(df_data.iloc[1].values,0,\"keyword\")\n",
    "columns2 = np.insert(df_data.iloc[2].values,0,\"keyword\")\n",
    "columns3 = np.insert(df_data.iloc[3].values,0,\"keyword\")\n",
    "columns4 = np.insert(df_data.iloc[4].values,0,\"keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_info.csv\",header=None).drop(0,axis=0).drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df0 = df[df[\"keyword\"] == \"0\"]\n",
    "df1 = df[df[\"keyword\"] == \"1\"]\n",
    "df2 = df[df[\"keyword\"] == \"2\"]\n",
    "df3 = df[df[\"keyword\"] == \"3\"]\n",
    "df4 = df[df[\"keyword\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_2_info.csv\",header=None).drop(0,axis=0).drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df2_0 = df[df[\"keyword\"] == \"0\"]\n",
    "df2_1 = df[df[\"keyword\"] == \"1\"]\n",
    "df2_2 = df[df[\"keyword\"] == \"2\"]\n",
    "df2_3 = df[df[\"keyword\"] == \"3\"]\n",
    "df2_4 = df[df[\"keyword\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uchidayuki/Python/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/uchidayuki/Python/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_3_info.csv\",header=None).drop(0,axis=0).drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df3_0 = df[df[\"keyword\"] == \"0\"]\n",
    "df3_1 = df[df[\"keyword\"] == \"1\"]\n",
    "df3_2 = df[df[\"keyword\"] == \"2\"]\n",
    "df3_3 = df[df[\"keyword\"] == \"3\"]\n",
    "df3_4 = df[df[\"keyword\"] == \"4\"]\n",
    "df3_2.loc[100] = [2, 5, 5, 4, 4, 5, 4, 4, 3, 1, 3, 3, 4, 4, 2, 5, 3, 4, 3, 4, 5, 5, 5, 2, 3, 3, 1, 2, 2, 2, 3, 1, 2, 4, 4, 2, 5, 4, 2, 2, 3, 5, 3, 4, 4, 5, 5, 3, 3, 4, 4]\n",
    "df3_2.loc[101] = [2, 5, 5, 4, 4, 5, 4, 4, 3, 1, 3, 3, 4, 4, 2, 5, 3, 4, 3, 4, 5, 5, 5, 2, 3, 3, 1, 2, 2, 2, 3, 1, 2, 4, 4, 2, 5, 4, 2, 2, 3, 5, 3, 4, 4, 5, 5, 3, 3, 4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns = columns0\n",
    "df1.columns = columns1\n",
    "df2.columns = columns2\n",
    "df3.columns = columns3\n",
    "df4.columns = columns4\n",
    "df2_0.columns = columns0\n",
    "df2_1.columns = columns1\n",
    "df2_2.columns = columns2\n",
    "df2_3.columns = columns3\n",
    "df2_4.columns = columns4\n",
    "df3_0.columns = columns0\n",
    "df3_1.columns = columns1\n",
    "df3_2.columns = columns2\n",
    "df3_3.columns = columns3\n",
    "df3_4.columns = columns4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df0 = pd.concat([df0, df2_0, df3_0]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df1 = pd.concat([df1, df2_1, df3_1]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df2 = pd.concat([df2, df2_2, df3_2]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df3 = pd.concat([df3, df2_3, df3_3]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df4 = pd.concat([df4, df2_4, df3_4]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均・分散算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df0_mean = pd.DataFrame(result_df0.mean()).T.round(1)\n",
    "result_df1_mean = pd.DataFrame(result_df1.mean()).T.round(1)\n",
    "result_df2_mean = pd.DataFrame(result_df2.mean()).T.round(1)\n",
    "result_df3_mean = pd.DataFrame(result_df3.mean()).T.round(1)\n",
    "result_df4_mean = pd.DataFrame(result_df4.mean()).T.round(1)\n",
    "\n",
    "keyword0_mean = pd.DataFrame(result_df0.mean().round(1)).T\n",
    "keyword0_std = pd.DataFrame(result_df0.std().round(1)).T\n",
    "keyword1_mean = pd.DataFrame(result_df1.mean().round(1)).T\n",
    "keyword1_std = pd.DataFrame(result_df1.std().round(1)).T\n",
    "keyword2_mean = pd.DataFrame(result_df2.mean().round(1)).T\n",
    "keyword2_std = pd.DataFrame(result_df2.std().round(1)).T\n",
    "keyword3_mean = pd.DataFrame(result_df3.mean().round(1)).T\n",
    "keyword3_std = pd.DataFrame(result_df3.std().round(1)).T\n",
    "keyword4_mean = pd.DataFrame(result_df4.mean().round(1)).T\n",
    "keyword4_std = pd.DataFrame(result_df4.std().round(1)).T\n",
    "\n",
    "\n",
    "keyword0 = pd.concat([keyword0_mean,keyword0_std])\n",
    "keyword0.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword1 = pd.concat([keyword1_mean,keyword1_std])\n",
    "keyword1.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword2 = pd.concat([keyword2_mean,keyword2_std])\n",
    "keyword2.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword3 = pd.concat([keyword3_mean,keyword3_std])\n",
    "keyword3.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword4 = pd.concat([keyword4_mean,keyword4_std])\n",
    "keyword4.index = [\"mean\", \"std\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    wakati = MeCab.Tagger(\"-O wakati\")\n",
    "    wakati.parse(\"\")\n",
    "    words = wakati.parse(text)\n",
    "\n",
    "    # Make word list\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = df0.columns[1:]\n",
    "texts0 = [tokenize(a) for a in samples0]\n",
    "samples1 = df1.columns[1:]\n",
    "texts1 = [tokenize(a) for a in samples1]\n",
    "samples2 = df2.columns[1:]\n",
    "texts2 = [tokenize(a) for a in samples2]\n",
    "samples3 = df3.columns[1:]\n",
    "texts3 = [tokenize(a) for a in samples3]\n",
    "samples4 = df4.columns[1:]\n",
    "texts4 = [tokenize(a) for a in samples4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "\n",
    "\n",
    "from Ocab import Ocab, Regexp\n",
    "c = Regexp()\n",
    "m = Ocab(target=[\"名詞\",\"動詞\",\"形容詞\",\"副詞\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unknown_token = np.random.randint(-1, 1, (200, 1))  #\n",
    "unknown_token = np.random.random_sample(200)\n",
    "word2vec_array0 = []\n",
    "for str in texts0:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array0.append(word_vectors)\n",
    "\n",
    "word2vec_array1 = []\n",
    "for str in texts1:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array1.append(word_vectors)\n",
    "\n",
    "word2vec_array2 = []\n",
    "for str in texts2:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array2.append(word_vectors)\n",
    "\n",
    "word2vec_array3 = []\n",
    "for str in texts3:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array3.append(word_vectors)\n",
    "\n",
    "\n",
    "word2vec_array4 = []\n",
    "for str in texts4:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(unknown_token)\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array4.append(word_vectors)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>久しぶりのお泊りディズニーで利用させていただきましたがほんとにお手頃な価格で、それだけで満足でした</th>\n",
       "      <th>女性の方でも安心です</th>\n",
       "      <th>2人での利用なのでお部屋もお広く清潔感もありゆったりと過ごすことができました、また利用させていただきたいと思います</th>\n",
       "      <th>当日、禁煙ルームへの変更をお願いしたところ12階にあるとても広い部屋に案内していただきました</th>\n",
       "      <th>キングサイズのベット、リビングルーム、トイレ、テレビも二ヶ所あり、大満足でした</th>\n",
       "      <th>毎月２日間、友人とディズニーを楽しんでおります</th>\n",
       "      <th>ホテルまでは駅からも徒歩で５分くらい行けるので便利です</th>\n",
       "      <th>送迎バスも有るので当日の疲れ具合や天候を加味して利用しております</th>\n",
       "      <th>ホテル内にコンビニも有るのでお水等を購入することも出来て便利です</th>\n",
       "      <th>今回はスタンダードの予約でしたが…キャンセルが出たようでグレードアップしていただきました</th>\n",
       "      <th>...</th>\n",
       "      <th>部屋の広さは  トランクも広げられて  ちょうど よいですね！</th>\n",
       "      <th>部屋も広く、パークが一望できるお部屋にしていただきました</th>\n",
       "      <th>とても綺麗でゆっくりできたのでまた利用したいです！</th>\n",
       "      <th>舞浜駅に行くには20分おきに無料バスがでていて便利でした</th>\n",
       "      <th>前から予約させてもらってたからか豪華なお部屋になってて嬉しかったです！</th>\n",
       "      <th>リーズナブルな価格で泊まれてとても満足です</th>\n",
       "      <th>立地がよく、電車で降りてから荷物を預けてユニバに行けたのがとても便利でした</th>\n",
       "      <th>ホテルのシャンプー、リンスも使い心地が良くてよかったです</th>\n",
       "      <th>チェックアウト後に朝からusjに行きました</th>\n",
       "      <th>荷物をロッカーに預けなければならず、探しに行こうと思っていたところ、チェックアウトしたにも関わらず、無料で預かってくれてとても助かりました</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      久しぶりのお泊りディズニーで利用させていただきましたがほんとにお手頃な価格で、それだけで満足でした  女性の方でも安心です  \\\n",
       "mean                                                4.7         2.8   \n",
       "std                                                 0.5         0.9   \n",
       "\n",
       "      2人での利用なのでお部屋もお広く清潔感もありゆったりと過ごすことができました、また利用させていただきたいと思います  \\\n",
       "mean                                                4.7           \n",
       "std                                                 0.5           \n",
       "\n",
       "      当日、禁煙ルームへの変更をお願いしたところ12階にあるとても広い部屋に案内していただきました  \\\n",
       "mean                                             4.8   \n",
       "std                                              0.4   \n",
       "\n",
       "      キングサイズのベット、リビングルーム、トイレ、テレビも二ヶ所あり、大満足でした  毎月２日間、友人とディズニーを楽しんでおります  \\\n",
       "mean                                      4.8                      3.7   \n",
       "std                                       0.4                      1.1   \n",
       "\n",
       "      ホテルまでは駅からも徒歩で５分くらい行けるので便利です  送迎バスも有るので当日の疲れ具合や天候を加味して利用しております  \\\n",
       "mean                          3.2                               3.7   \n",
       "std                           0.8                               1.1   \n",
       "\n",
       "      ホテル内にコンビニも有るのでお水等を購入することも出来て便利です  \\\n",
       "mean                               3.8   \n",
       "std                                1.1   \n",
       "\n",
       "      今回はスタンダードの予約でしたが…キャンセルが出たようでグレードアップしていただきました  \\\n",
       "mean                                           4.3   \n",
       "std                                            0.8   \n",
       "\n",
       "                                      ...                                    \\\n",
       "mean                                  ...                                     \n",
       "std                                   ...                                     \n",
       "\n",
       "      部屋の広さは  トランクも広げられて  ちょうど よいですね！   部屋も広く、パークが一望できるお部屋にしていただきました  \\\n",
       "mean                               3.6                           4.1   \n",
       "std                                1.2                           0.8   \n",
       "\n",
       "      とても綺麗でゆっくりできたのでまた利用したいです！  舞浜駅に行くには20分おきに無料バスがでていて便利でした  \\\n",
       "mean                        4.3                           4.3   \n",
       "std                         1.0                           0.7   \n",
       "\n",
       "      前から予約させてもらってたからか豪華なお部屋になってて嬉しかったです！  リーズナブルな価格で泊まれてとても満足です  \\\n",
       "mean                                  4.4                    4.2   \n",
       "std                                   1.0                    1.0   \n",
       "\n",
       "      立地がよく、電車で降りてから荷物を預けてユニバに行けたのがとても便利でした  ホテルのシャンプー、リンスも使い心地が良くてよかったです  \\\n",
       "mean                                    4.4                           4.6   \n",
       "std                                     1.0                           0.7   \n",
       "\n",
       "      チェックアウト後に朝からusjに行きました  \\\n",
       "mean                    4.1   \n",
       "std                     1.1   \n",
       "\n",
       "      荷物をロッカーに預けなければならず、探しに行こうと思っていたところ、チェックアウトしたにも関わらず、無料で預かってくれてとても助かりました  \n",
       "mean                                                4.8                      \n",
       "std                                                 0.4                      \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##これが文章ベクトル 入力データ\n",
    "word2vec_array0\n",
    "word2vec_array1\n",
    "word2vec_array2\n",
    "word2vec_array3\n",
    "word2vec_array4\n",
    "##これが正解データ\n",
    "keyword0\n",
    "keyword1\n",
    "keyword2\n",
    "keyword3\n",
    "keyword4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [文章ベクトル,正解データ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[],[],[]]\n",
    "for vec, label, row in zip(word2vec_array0, keyword0.loc[\"mean\"].values, samples0):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array1, keyword1.loc[\"mean\"].values, samples1):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array2, keyword2.loc[\"mean\"].values, samples2):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array3, keyword3.loc[\"mean\"].values, samples3):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array4, keyword4.loc[\"mean\"].values, samples4):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('input_data_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力長は25に決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_length = 25\n",
    "input_data = []\n",
    "for vector in data[0]:\n",
    "    context_vectors = vector\n",
    "    while(len(context_vectors)<input_length):\n",
    "        context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#     print(context_vectors)\n",
    "    input_data.append(context_vectors)\n",
    "output_data = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data)\n",
    "# train_input_data, val_input_data, train_output_data, val_output_data = train_test_split(input_data,output_data,train_size=0.8, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "# print(input_data.shape)\n",
    "# print(output_data.shape)\n",
    "# print(len(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 25, 200) (250,) 250\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_49 (LSTM)               (None, 512)               1460224   \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,460,737\n",
      "Trainable params: 1,460,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras import backend as K \n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_train = int(len(input_data) * 0.8)\n",
    "N_validation = len(input_data) - N_train\n",
    "\n",
    "print(input_data.shape,output_data.shape, len(data[2]))\n",
    "X_train, X_validation, Y_train, Y_validation, row_train, row_validation = \\\n",
    "    train_test_split(input_data, output_data, data[2], test_size=N_validation)\n",
    "\n",
    "\n",
    "maxlen = 25\n",
    "'''\n",
    "モデル設定\n",
    "'''\n",
    "n_in = 200  \n",
    "n_hidden = 512\n",
    "n_out = 1\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred): \n",
    "     return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden,\n",
    "               kernel_initializer=weight_variable,\n",
    "               input_shape=(maxlen, n_in)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# optimizer = Adam(lr=0.0010, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "# model.compile(loss='mean_squared_error',optimizer=optimizer)\n",
    "model.compile(loss=root_mean_squared_error,\n",
    "              optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/300\n",
      "200/200 [==============================] - 12s 58ms/step - loss: 3.5004 - val_loss: 1.7295\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 1.2666 - val_loss: 0.5472\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5785 - val_loss: 0.4522\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5370 - val_loss: 0.4652\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5348 - val_loss: 0.4759\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5445 - val_loss: 0.4590\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5255 - val_loss: 0.4581\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.5480 - val_loss: 0.4516\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5256 - val_loss: 0.4682\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5399 - val_loss: 0.4689\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5498 - val_loss: 0.4540\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.6006 - val_loss: 0.4460\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5252 - val_loss: 0.4559\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5346 - val_loss: 0.4572\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5635 - val_loss: 0.4604\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5453 - val_loss: 0.4480\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5360 - val_loss: 0.4532\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5592 - val_loss: 0.4453\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5253 - val_loss: 0.4482\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5172 - val_loss: 0.4552\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5382 - val_loss: 0.4568\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5588 - val_loss: 0.4447\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5541 - val_loss: 0.4450\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5416 - val_loss: 0.4412\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5310 - val_loss: 0.4417\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5237 - val_loss: 0.4774\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.5288 - val_loss: 0.4899\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.5477 - val_loss: 0.4669\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5109 - val_loss: 0.4534\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5285 - val_loss: 0.4654\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4962 - val_loss: 0.4677\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5433 - val_loss: 0.5275\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5657 - val_loss: 0.4482\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.4915 - val_loss: 0.4363\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5102 - val_loss: 0.4290\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5085 - val_loss: 0.4154\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4892 - val_loss: 0.4158\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4670 - val_loss: 0.4126\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4935 - val_loss: 0.4243\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4983 - val_loss: 0.4442\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.5248 - val_loss: 0.3887\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.4584 - val_loss: 0.4083\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4194 - val_loss: 0.4335\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4461 - val_loss: 0.5438\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5244 - val_loss: 0.4606\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4376 - val_loss: 0.3699\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4508 - val_loss: 0.3977\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4456 - val_loss: 0.3833\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4033 - val_loss: 0.4465\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3956 - val_loss: 0.4108\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3971 - val_loss: 0.3963\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3838 - val_loss: 0.4158\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4400 - val_loss: 0.4744\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4364 - val_loss: 0.3747\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3924 - val_loss: 0.4101\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3648 - val_loss: 0.3770\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4322 - val_loss: 0.4018\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3832 - val_loss: 0.3687\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3615 - val_loss: 0.4283\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4476 - val_loss: 0.4419\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3961 - val_loss: 0.4323\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.4397 - val_loss: 0.3928\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3736 - val_loss: 0.3519\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3775 - val_loss: 0.3743\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3778 - val_loss: 0.4219\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3786 - val_loss: 0.3605\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3488 - val_loss: 0.3704\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3257 - val_loss: 0.3655\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3337 - val_loss: 0.3978\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3562 - val_loss: 0.3775\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3661 - val_loss: 0.3408\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3130 - val_loss: 0.3798\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3490 - val_loss: 0.4978\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4338 - val_loss: 0.4065\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3615 - val_loss: 0.3645\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3437 - val_loss: 0.3979\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3746 - val_loss: 0.3835\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3230 - val_loss: 0.3815\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3091 - val_loss: 0.4065\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3189 - val_loss: 0.4187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3609 - val_loss: 0.4204\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3111 - val_loss: 0.4052\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3050 - val_loss: 0.3856\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2822 - val_loss: 0.3900\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3700 - val_loss: 0.4894\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.3236 - val_loss: 0.4273\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3413 - val_loss: 0.4010\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.3100 - val_loss: 0.4683\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3258 - val_loss: 0.4077\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2829 - val_loss: 0.5022\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2790 - val_loss: 0.3896\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2682 - val_loss: 0.4775\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3564 - val_loss: 0.4077\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2738 - val_loss: 0.4335\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2813 - val_loss: 0.4612\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.3009 - val_loss: 0.3692\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2679 - val_loss: 0.3659\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2425 - val_loss: 0.3621\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2553 - val_loss: 0.4776\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3804 - val_loss: 0.4855\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3229 - val_loss: 0.4627\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2538 - val_loss: 0.4062\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2313 - val_loss: 0.3961\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2503 - val_loss: 0.3795\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3107 - val_loss: 0.4583\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3379 - val_loss: 0.4147\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.3130 - val_loss: 0.4640\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2809 - val_loss: 0.3858\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2322 - val_loss: 0.4052\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2167 - val_loss: 0.4818\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2785 - val_loss: 0.4971\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2591 - val_loss: 0.6016\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3023 - val_loss: 0.4767\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2799 - val_loss: 0.4452\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2777 - val_loss: 0.4215\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2629 - val_loss: 0.4209\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2287 - val_loss: 0.4411\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2278 - val_loss: 0.4143\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2407 - val_loss: 0.4340\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2522 - val_loss: 0.4047\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2794 - val_loss: 0.3892\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2459 - val_loss: 0.4004\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2146 - val_loss: 0.3854\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2016 - val_loss: 0.4412\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1906 - val_loss: 0.4342\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2735 - val_loss: 0.6233\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3652 - val_loss: 0.4283\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2645 - val_loss: 0.4604\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2573 - val_loss: 0.4540\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.4277\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2661 - val_loss: 0.5332\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2638 - val_loss: 0.4598\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2362 - val_loss: 0.4525\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2482 - val_loss: 0.4584\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2454 - val_loss: 0.4253\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2187 - val_loss: 0.4030\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2259 - val_loss: 0.4037\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2339 - val_loss: 0.4773\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2195 - val_loss: 0.4671\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2235 - val_loss: 0.5353\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2887 - val_loss: 0.4287\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2193 - val_loss: 0.4420\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2030 - val_loss: 0.4128\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.1911 - val_loss: 0.4445\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2417 - val_loss: 0.5382\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2857 - val_loss: 0.4215\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2117 - val_loss: 0.4214\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2311 - val_loss: 0.4413\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2764 - val_loss: 0.4391\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.2653 - val_loss: 0.4226\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2331 - val_loss: 0.4260\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2204 - val_loss: 0.3729\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1999 - val_loss: 0.3531\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.1976 - val_loss: 0.3892\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.2644 - val_loss: 0.3813\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 2s 9ms/step - loss: 0.2399 - val_loss: 0.3790\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2259 - val_loss: 0.3903\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2667 - val_loss: 0.3801\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2302 - val_loss: 0.3635\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1924 - val_loss: 0.3829\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2548 - val_loss: 0.3668\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2115 - val_loss: 0.4110\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2303 - val_loss: 0.4122\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2330 - val_loss: 0.4463\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2296 - val_loss: 0.3889\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1870 - val_loss: 0.4220\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2290 - val_loss: 0.3919\n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1633 - val_loss: 0.4295\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1849 - val_loss: 0.4320\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1947 - val_loss: 0.3706\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2039 - val_loss: 0.3777\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2576 - val_loss: 0.4020\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2296 - val_loss: 0.4197\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2322 - val_loss: 0.4222\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2353 - val_loss: 0.3939\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2314 - val_loss: 0.3783\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2186 - val_loss: 0.3989\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2159 - val_loss: 0.3990\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1979 - val_loss: 0.3664\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1737 - val_loss: 0.3656\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1986 - val_loss: 0.3843\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2281 - val_loss: 0.4337\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2204 - val_loss: 0.3898\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2464 - val_loss: 0.3955\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2572 - val_loss: 0.3757\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2315 - val_loss: 0.3844\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2110 - val_loss: 0.3744\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2002 - val_loss: 0.4008\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2223 - val_loss: 0.3799\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2319 - val_loss: 0.3658\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2027 - val_loss: 0.3613\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1836 - val_loss: 0.3697\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1889 - val_loss: 0.3923\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2246 - val_loss: 0.3834\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2365 - val_loss: 0.3984\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2155 - val_loss: 0.4198\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2294 - val_loss: 0.3544\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1810 - val_loss: 0.3579\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1956 - val_loss: 0.3772\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1857 - val_loss: 0.3844\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2191 - val_loss: 0.4331\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2280 - val_loss: 0.4044\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2236 - val_loss: 0.3925\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2154 - val_loss: 0.3772\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2017 - val_loss: 0.3767\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1891 - val_loss: 0.3893\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1681 - val_loss: 0.4014\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2583 - val_loss: 0.4251\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2491 - val_loss: 0.3999\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.2184 - val_loss: 0.3855\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1953 - val_loss: 0.3928\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2061 - val_loss: 0.3779\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1739 - val_loss: 0.3836\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1965 - val_loss: 0.4063\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2612 - val_loss: 0.3681\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1840 - val_loss: 0.3834\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1714 - val_loss: 0.3926\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2194 - val_loss: 0.4430\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2422 - val_loss: 0.4400\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2208 - val_loss: 0.3804\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1952 - val_loss: 0.3941\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2194 - val_loss: 0.4290\n",
      "Epoch 223/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2261 - val_loss: 0.3961\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2199 - val_loss: 0.4238\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.4114\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.2108 - val_loss: 0.3882\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1590 - val_loss: 0.3996\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1640 - val_loss: 0.4337\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1798 - val_loss: 0.4266\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2724 - val_loss: 0.4392\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2318 - val_loss: 0.4197\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2150 - val_loss: 0.3911\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.4300\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2151 - val_loss: 0.4563\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2272 - val_loss: 0.4580\n",
      "Epoch 236/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2398 - val_loss: 0.3989\n",
      "Epoch 237/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1825 - val_loss: 0.3751\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1561 - val_loss: 0.4136\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1827 - val_loss: 0.5242\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2737 - val_loss: 0.4552\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2045 - val_loss: 0.4360\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1644 - val_loss: 0.4402\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1807 - val_loss: 0.4467\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2184 - val_loss: 0.4744\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2096 - val_loss: 0.4321\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 1s 4ms/step - loss: 0.1852 - val_loss: 0.4335\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1933 - val_loss: 0.4661\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2016 - val_loss: 0.4231\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2028 - val_loss: 0.4215\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1814 - val_loss: 0.4493\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2109 - val_loss: 0.5273\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2333 - val_loss: 0.4361\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.1986 - val_loss: 0.4132\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1687 - val_loss: 0.4493\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1822 - val_loss: 0.4557\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.2136 - val_loss: 0.4948\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2040 - val_loss: 0.4705\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2007 - val_loss: 0.4343\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1660 - val_loss: 0.4030\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 0.1786 - val_loss: 0.4452\n",
      "Epoch 261/300\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 0.1719"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-666-e5f6352d546a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           validation_data=(X_validation, Y_validation))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Python/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "モデル学習\n",
    "'''\n",
    "epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(X_validation, Y_validation),\n",
    "#           callbacks=[early_stopping])\n",
    "\n",
    "#earlystoppingなし\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_validation, Y_validation))\n",
    "\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(X_validation, Y_validation))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_length = 25\n",
    "# test_vectors = [] \n",
    "# len(data[0])\n",
    "# for v in data[0]:\n",
    "#     context_vectors = v\n",
    "#     if len(v) == 14:\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         test_vectors.append(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vectors = np.array(test_vectors)\n",
    "y_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_validation)\n",
    "# for text, pred in zip(row_train, y_train_pred):\n",
    "#     print(text, pred)\n",
    "\n",
    "for text, pred, ans in zip(row_validation, y_test_pred, Y_validation):\n",
    "    print(text, pred, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_validation, Y_validation)\n",
    "# ほぼR^2 = 0.5なので、|R|は0.69999で\n",
    "# 1.0≧|R|≧0.7 ：高い相関がある \n",
    "# 0.7≧|R|≧0.5 ：かなり高い相関がある \n",
    "# に当てはまるので、まぁそこそこ？\n",
    "\n",
    "\n",
    "# LR = 0.0005\n",
    "\n",
    "#なし 0.42 0.37 epochs 100 \n",
    "#なし 0.35 0.38 epochs 200 \n",
    "\n",
    "# dropout 0.2 0.35 0.35 epochs 100 \n",
    "# dropout 0.2 0.35 0.35 epochs 200 \n",
    "\n",
    "# recurrent_dropout=0.2, 0.35 0.37 epochs 100\n",
    "\n",
    "# reccurent_dropout0.5 = 0.566 \n",
    "# reccurent_dropout0.5 = 0.37 0.38 epochs 100 \n",
    "# reccurent_dropout0.5 = 0.38 0.42 0.38 epochs 200\n",
    "\n",
    "\n",
    "# recurrent_dropout=0.2, dropout=0.2, 0.41 =>0.36 =>0.41 epochs100\n",
    "# recurrent_dropout=0.2, dropout=0.2, 0.36 => 0.41 => 0.40 epochs200\n",
    "\n",
    "\n",
    "# LR = 0.001\n",
    "# なし  0.39 0.44 0.39 epochs 200\n",
    "\n",
    "# なし　0.54 0.49 0.42  epochs 300\n",
    "# なし  0.46 0.43 0.38   epochs 400\n",
    "# recurrent_dropout=0.2  0.41   epochs 300\n",
    "\n",
    "\n",
    "# dropout=0.5  0.32 0.40  epochs 300\n",
    "\n",
    "\n",
    "\n",
    "#DROPOUT = 0.3  0.44 epochs 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intlen(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hidden256 dropout0.3が今の所一番精度が高い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adam epochs350が良さげ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "出力を用いて予測\n",
    "'''\n",
    "# 元データの最初の一部だけ切り出し\n",
    "# model.predict()\n",
    "# input_data[3:10].shape\n",
    "\n",
    "test_text = [ \"昨日、バイトがあって、お客さんがたくさん来ました\",\"浅井さんは丁寧で、非常に優しい\",\"前にここに来たときは、大雪だったので、あまり楽しめませんでした\",\"宿は長野県にある\",\"この旅館は人気がある\",\"従業員の方々は非常に優しく、対応が丁寧です\",\"従業員の方々が優しかったです\",\"佐々木希が来たらしい\",\"料理がとっても美味しくて、最高でした！\",\"非常に景色と温泉が綺麗で、部屋の内装やアメニティが良く、非常に満足出来るものでした\"]\n",
    "test_wakati_text = []\n",
    "for text in test_text:\n",
    "    test_wakati_text.append(tokenize(text))\n",
    "\n",
    "# test_wakati_text = \"\"\n",
    "word_model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "test_vectors = []\n",
    "for wakati_text in test_wakati_text:\n",
    "    test_vector = []\n",
    "    for char in m.removeStoplist(wakati_text, []).split(\" \"):\n",
    "        try:\n",
    "            test_vector.append(word_model[char])\n",
    "        except KeyError:\n",
    "            test_vector.append(unknown_token)\n",
    "    test_vectors.append(np.array(test_vector))\n",
    "\n",
    "# test_vectors = np.array(test_vectors)\n",
    "input_length = 25\n",
    "input_test_data = []\n",
    "for test_vector in test_vectors:\n",
    "    context_vectors = test_vector\n",
    "    while(len(context_vectors)<input_length):\n",
    "        context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "    input_test_data.append(context_vectors)\n",
    "input_test_data = np.array(input_test_data)\n",
    "\n",
    "\n",
    "test_predictions = model.predict(input_test_data)\n",
    "\n",
    "for pred, text in zip(test_predictions, test_text):\n",
    "    print(text, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### めも\n",
    "\n",
    "文章の情報量が対話継続欲求に影響して言えるのではないか。\n",
    "\n",
    "文章の情報量\n",
    "\n",
    "複数のモデルを作る\n",
    "\n",
    "\n",
    "綺麗でした　をそのまま学習したモデルと\n",
    "綺麗でした　を[0,2,0.0001,0,000002]情報量(単語頻度)で変換させた後学習したモデルを比較すれば情報量が影響しているかどうかがわかる\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 重要\n",
    "- もともと、経験談の感じ方は人によって、違い、分散が大きく出るものもある。なので、分散が大きいことも考慮して、分散が大きいだろうと予測できるものは予測の数値を下げるか、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
