{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "test_text = \"学生時代の友人7名で利用させて頂きました\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "test_text_wakati = tagger.parse(test_text).split(\" \")\n",
    "# test_text_wakati\n",
    "# for str in test_text_wakati:\n",
    "#     print(model['str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_data = pd.read_csv(\"./some.csv\",encoding='cp932',header=None)\n",
    "df_data = df_data.drop(0,axis=0).drop(1,axis=1)\n",
    "df_data = df_data.drop(0,axis=1)\n",
    "columns0 = df_data.iloc[0].values\n",
    "columns0 = np.insert(columns0,0,\"keyword\")\n",
    "columns1 = df_data.iloc[1].values\n",
    "columns1 = np.insert(columns1,0,\"keyword\")\n",
    "columns2 = df_data.iloc[2].values\n",
    "columns2 = np.insert(columns2,0,\"keyword\")\n",
    "columns3 = df_data.iloc[3].values\n",
    "columns3 = np.insert(columns3,0,\"keyword\")\n",
    "columns4 = df_data.iloc[4].values\n",
    "columns4 = np.insert(columns4,0,\"keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_info.csv\",header=None)\n",
    "df = df.drop(0,axis=0)\n",
    "df = df.drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df0 = df[df[\"keyword\"] == \"0\"]\n",
    "df1 = df[df[\"keyword\"] == \"1\"]\n",
    "df2 = df[df[\"keyword\"] == \"2\"]\n",
    "df3 = df[df[\"keyword\"] == \"3\"]\n",
    "df4 = df[df[\"keyword\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_2_info.csv\",header=None)\n",
    "df = df.drop(0,axis=0)\n",
    "df = df.drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df2_0 = df[df[\"keyword\"] == \"0\"]\n",
    "df2_1 = df[df[\"keyword\"] == \"1\"]\n",
    "df2_2 = df[df[\"keyword\"] == \"2\"]\n",
    "df2_3 = df[df[\"keyword\"] == \"3\"]\n",
    "df2_4 = df[df[\"keyword\"] == \"4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uchidayuki/Python/lib/python3.6/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/uchidayuki/Python/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    keyword  1  2  3  4  5  6  7  8  9 ... 41 42 43 44 45 46 47 48 49 50\n",
       "17        2  5  5  3  1  5  5  5  3  5 ...  5  5  5  5  5  5  5  5  5  5\n",
       "25        2  4  5  3  5  5  4  4  3  3 ...  5  5  5  5  5  5  5  5  5  5\n",
       "100       2  5  5  4  4  5  4  4  3  1 ...  5  3  4  4  5  5  3  3  4  4\n",
       "101       2  5  5  4  4  5  4  4  3  1 ...  5  3  4  4  5  5  3  3  4  4\n",
       "\n",
       "[4 rows x 51 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./kikuchi_sotsuken/finish_3_info.csv\",header=None)\n",
    "df = df.drop(0,axis=0)\n",
    "df = df.drop([0,1,2,3],axis=1)\n",
    "retu = []\n",
    "for i in range(105):\n",
    "    if (i%2 == 0)and(i >=6):\n",
    "        retu.append(i)\n",
    "df = df.drop(retu,axis=1)\n",
    "columns = []\n",
    "for i in range(51):\n",
    "    if i == 0:\n",
    "        columns.append(\"keyword\")\n",
    "    else:\n",
    "        columns.append(i)\n",
    "df.columns = columns\n",
    "df3_0 = df[df[\"keyword\"] == \"0\"]\n",
    "df3_1 = df[df[\"keyword\"] == \"1\"]\n",
    "df3_2 = df[df[\"keyword\"] == \"2\"]\n",
    "df3_3 = df[df[\"keyword\"] == \"3\"]\n",
    "df3_4 = df[df[\"keyword\"] == \"4\"]\n",
    "df3_2.loc[100] = [2, 5, 5, 4, 4, 5, 4, 4, 3, 1, 3, 3, 4, 4, 2, 5, 3, 4, 3, 4, 5, 5, 5, 2, 3, 3, 1, 2, 2, 2, 3, 1, 2, 4, 4, 2, 5, 4, 2, 2, 3, 5, 3, 4, 4, 5, 5, 3, 3, 4, 4]\n",
    "df3_2.loc[101] = [2, 5, 5, 4, 4, 5, 4, 4, 3, 1, 3, 3, 4, 4, 2, 5, 3, 4, 3, 4, 5, 5, 5, 2, 3, 3, 1, 2, 2, 2, 3, 1, 2, 4, 4, 2, 5, 4, 2, 2, 3, 5, 3, 4, 4, 5, 5, 3, 3, 4, 4]\n",
    "df3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df0.columns = columns0\n",
    "df1.columns = columns1\n",
    "df2.columns = columns2\n",
    "df3.columns = columns3\n",
    "df4.columns = columns4\n",
    "df2_0.columns = columns0\n",
    "df2_1.columns = columns1\n",
    "df2_2.columns = columns2\n",
    "df2_3.columns = columns3\n",
    "df2_4.columns = columns4\n",
    "df3_0.columns = columns0\n",
    "df3_1.columns = columns1\n",
    "df3_2.columns = columns2\n",
    "df3_3.columns = columns3\n",
    "df3_4.columns = columns4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df0 = pd.concat([df0, df2_0, df3_0]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df1 = pd.concat([df1, df2_1, df3_1]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df2 = pd.concat([df2, df2_2, df3_2]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df3 = pd.concat([df3, df2_3, df3_3]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)\n",
    "result_df4 = pd.concat([df4, df2_4, df3_4]).drop(\"keyword\",axis=1).reset_index(drop=True).astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均・分散算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>子ども用のスリッパが置いてあるとありがたいです</th>\n",
       "      <th>始めて、家族風呂を利用させて貰いましたが、清潔でとても気持ち良かったです</th>\n",
       "      <th>ご飯もとても美味しく、いつもは小食の子どももたくさん食べました</th>\n",
       "      <th>フロントさんやスタッフさんもとても親切で、楽しい家族旅行となりました</th>\n",
       "      <th>梅雨時の大雨の真っ最中、部屋は山側でしたが多分海側だとしても霧がすごかったのでどっちでも同じだったと思います</th>\n",
       "      <th>低階層とのことでしたが、グレードアップしていただいたのでしょうか、高階層でした</th>\n",
       "      <th>でも外の景色を堪能出来ないほど遊び疲れました（笑）霧がだいぶ深いせいか、却ってアクアガーデンのショーは綺麗に見えました</th>\n",
       "      <th>梅雨時は人も少なく、でも遊ぶのは屋内だし、棚湯も雨でもさほど気になりませんでした</th>\n",
       "      <th>思った以上に外国人スタッフが多く、海外から来る人には馴染みやすいのかもですね</th>\n",
       "      <th>開放的な三角の屋根の天井で、ログハウスを満喫できます。</th>\n",
       "      <th>...</th>\n",
       "      <th>新潟方面を回ってやってきて用向きは渋谷、帰宅は羽田だったので地理的に品川が便利で決めています</th>\n",
       "      <th>せっかくのマッサージチェアでしたが、なんとなく使いませんでした</th>\n",
       "      <th>丁度台風接近の最中お手数おかけしましたが、こちらに宿泊出来たこととても感謝しています</th>\n",
       "      <th>私も機会がありましたら是非とも利用させて頂きたいと思いました</th>\n",
       "      <th>また利用したいと思える素晴らしいホテルでした</th>\n",
       "      <th>立地も良く、サービスも良く、店内のレストランやコンビニもOK</th>\n",
       "      <th>快適に宿泊させて頂きましたが・・・部屋に携帯の充電器を忘れてたと思いますが？また、連絡をさせて頂きます</th>\n",
       "      <th>いつも出張で宿泊していて素晴らしいので、今回は都内に住む孫と会うために利用しました</th>\n",
       "      <th>チェックインの時に、孫のためにタオルもご用意いただき、そのご配慮に感動しました</th>\n",
       "      <th>部屋からの眺望も素晴らしく、孫はずっと外を見てました</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      子ども用のスリッパが置いてあるとありがたいです  始めて、家族風呂を利用させて貰いましたが、清潔でとても気持ち良かったです  \\\n",
       "mean                      2.7                                   4.7   \n",
       "std                       1.4                                   0.8   \n",
       "\n",
       "      ご飯もとても美味しく、いつもは小食の子どももたくさん食べました  フロントさんやスタッフさんもとても親切で、楽しい家族旅行となりました  \\\n",
       "mean                              4.6                                 4.7   \n",
       "std                               1.1                                 0.8   \n",
       "\n",
       "      梅雨時の大雨の真っ最中、部屋は山側でしたが多分海側だとしても霧がすごかったのでどっちでも同じだったと思います  \\\n",
       "mean                                                3.9        \n",
       "std                                                 1.0        \n",
       "\n",
       "      低階層とのことでしたが、グレードアップしていただいたのでしょうか、高階層でした  \\\n",
       "mean                                      4.0   \n",
       "std                                       1.0   \n",
       "\n",
       "      でも外の景色を堪能出来ないほど遊び疲れました（笑）霧がだいぶ深いせいか、却ってアクアガーデンのショーは綺麗に見えました  \\\n",
       "mean                                                4.4             \n",
       "std                                                 1.1             \n",
       "\n",
       "      梅雨時は人も少なく、でも遊ぶのは屋内だし、棚湯も雨でもさほど気になりませんでした  \\\n",
       "mean                                       4.3   \n",
       "std                                        0.8   \n",
       "\n",
       "      思った以上に外国人スタッフが多く、海外から来る人には馴染みやすいのかもですね  開放的な三角の屋根の天井で、ログハウスを満喫できます。   \\\n",
       "mean                                     3.5                           2.8   \n",
       "std                                      0.9                           1.3   \n",
       "\n",
       "                 ...              \\\n",
       "mean             ...               \n",
       "std              ...               \n",
       "\n",
       "      新潟方面を回ってやってきて用向きは渋谷、帰宅は羽田だったので地理的に品川が便利で決めています  \\\n",
       "mean                                             3.1   \n",
       "std                                              1.1   \n",
       "\n",
       "      せっかくのマッサージチェアでしたが、なんとなく使いませんでした  \\\n",
       "mean                              3.2   \n",
       "std                               1.2   \n",
       "\n",
       "      丁度台風接近の最中お手数おかけしましたが、こちらに宿泊出来たこととても感謝しています  \\\n",
       "mean                                         3.8   \n",
       "std                                          0.9   \n",
       "\n",
       "      私も機会がありましたら是非とも利用させて頂きたいと思いました  また利用したいと思える素晴らしいホテルでした  \\\n",
       "mean                             2.4                     4.0   \n",
       "std                              1.4                     0.6   \n",
       "\n",
       "      立地も良く、サービスも良く、店内のレストランやコンビニもOK  \\\n",
       "mean                             3.1   \n",
       "std                              1.3   \n",
       "\n",
       "      快適に宿泊させて頂きましたが・・・部屋に携帯の充電器を忘れてたと思いますが？また、連絡をさせて頂きます  \\\n",
       "mean                                                3.6     \n",
       "std                                                 1.2     \n",
       "\n",
       "      いつも出張で宿泊していて素晴らしいので、今回は都内に住む孫と会うために利用しました  \\\n",
       "mean                                        3.8   \n",
       "std                                         1.1   \n",
       "\n",
       "      チェックインの時に、孫のためにタオルもご用意いただき、そのご配慮に感動しました  部屋からの眺望も素晴らしく、孫はずっと外を見てました  \n",
       "mean                                      4.2                         4.2  \n",
       "std                                       1.1                         1.2  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df0_mean = pd.DataFrame(result_df0.mean()).T.round(1)\n",
    "result_df1_mean = pd.DataFrame(result_df1.mean()).T.round(1)\n",
    "result_df2_mean = pd.DataFrame(result_df2.mean()).T.round(1)\n",
    "result_df3_mean = pd.DataFrame(result_df3.mean()).T.round(1)\n",
    "result_df4_mean = pd.DataFrame(result_df4.mean()).T.round(1)\n",
    "\n",
    "keyword0_mean = pd.DataFrame(result_df0.mean().round(1)).T\n",
    "keyword0_std = pd.DataFrame(result_df0.std().round(1)).T\n",
    "keyword1_mean = pd.DataFrame(result_df1.mean().round(1)).T\n",
    "keyword1_std = pd.DataFrame(result_df1.std().round(1)).T\n",
    "keyword2_mean = pd.DataFrame(result_df2.mean().round(1)).T\n",
    "keyword2_std = pd.DataFrame(result_df2.std().round(1)).T\n",
    "keyword3_mean = pd.DataFrame(result_df3.mean().round(1)).T\n",
    "keyword3_std = pd.DataFrame(result_df3.std().round(1)).T\n",
    "keyword4_mean = pd.DataFrame(result_df4.mean().round(1)).T\n",
    "keyword4_std = pd.DataFrame(result_df4.std().round(1)).T\n",
    "\n",
    "\n",
    "keyword0 = pd.concat([keyword0_mean,keyword0_std])\n",
    "keyword0.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword1 = pd.concat([keyword1_mean,keyword1_std])\n",
    "keyword1.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword2 = pd.concat([keyword2_mean,keyword2_std])\n",
    "keyword2.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword3 = pd.concat([keyword3_mean,keyword3_std])\n",
    "keyword3.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword4 = pd.concat([keyword4_mean,keyword4_std])\n",
    "keyword4.index = [\"mean\", \"std\"]\n",
    "\n",
    "keyword1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    wakati = MeCab.Tagger(\"-O wakati\")\n",
    "    wakati.parse(\"\")\n",
    "    words = wakati.parse(text)\n",
    "\n",
    "    # Make word list\n",
    "    if words[-1] == u\"\\n\":\n",
    "        words = words[:-1]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = df0.columns[1:]\n",
    "texts0 = [tokenize(a) for a in samples0]\n",
    "samples1 = df1.columns[1:]\n",
    "texts1 = [tokenize(a) for a in samples1]\n",
    "samples2 = df2.columns[1:]\n",
    "texts2 = [tokenize(a) for a in samples2]\n",
    "samples3 = df3.columns[1:]\n",
    "texts3 = [tokenize(a) for a in samples3]\n",
    "samples4 = df4.columns[1:]\n",
    "texts4 = [tokenize(a) for a in samples4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "\n",
    "\n",
    "from Ocab import Ocab, Regexp\n",
    "c = Regexp()\n",
    "m = Ocab(target=[\"名詞\",\"動詞\",\"形容詞\",\"副詞\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = np.random.randint(-1, 1, (200, 1))  #\n",
    "word2vec_array0 = []\n",
    "for str in texts0:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(np.random.random_sample(200))\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array0.append(word_vectors)\n",
    "\n",
    "word2vec_array1 = []\n",
    "for str in texts1:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(np.random.random_sample(200))\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array1.append(word_vectors)\n",
    "\n",
    "word2vec_array2 = []\n",
    "for str in texts2:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(np.random.random_sample(200))\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array2.append(word_vectors)\n",
    "\n",
    "word2vec_array3 = []\n",
    "for str in texts3:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(np.random.random_sample(200))\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array3.append(word_vectors)\n",
    "\n",
    "\n",
    "word2vec_array4 = []\n",
    "for str in texts4:\n",
    "    word_vectors = []\n",
    "    for char in m.removeStoplist(str, []).split(\" \"):\n",
    "        try:\n",
    "            word_vectors.append(model[char])\n",
    "#             print('#{0}ok'.format(char))\n",
    "        except KeyError:\n",
    "#             print('#{0}vocabularry error'.format(char))\n",
    "            for c in tokenize(char).split(\" \"):\n",
    "                try:\n",
    "                    word_vectors.append(model[c])\n",
    "#                     print('#{0}ok'.format(c))\n",
    "                except KeyError:\n",
    "                    word_vectors.append(np.random.random_sample(200))\n",
    "#                     print('#{0}vocabularry error'.format(c))\n",
    "    word2vec_array4.append(word_vectors)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>学生時代の友人7名で利用させて頂きました</th>\n",
       "      <th>ちょうど桜が咲いていて、別荘地内の桜もライトアップされていて、雰囲気良かったです</th>\n",
       "      <th>信州安曇野、穂高温泉郷に位置する本格ログハウスの貸別荘です</th>\n",
       "      <th>コテージ内はとても清潔感があって安心して泊まれました</th>\n",
       "      <th>インターから遠くもなく、コテージの近くにはコンビニや買い出しできるところもあるので、立地的にも満足でした</th>\n",
       "      <th>趣味で一棟貸しのところをいろいろ利用していますがこちらの設備環境はとても清潔で周辺もとても静かなところで居心地が大変良かったです</th>\n",
       "      <th>到着した日や翌日は雪の多い日でしたがスタッフの方が車や周辺の除雪をしてくれて大変助かりました</th>\n",
       "      <th>また時間があれば利用させていただきたいと思います</th>\n",
       "      <th>ちょうど紅葉が綺麗な季節に滞在できました</th>\n",
       "      <th>チェックインする前から、暖房がついていたりして、オーナーの気遣いを感じることができました</th>\n",
       "      <th>...</th>\n",
       "      <th>「棚湯」って何？　と思っていたが、屋上に棚田のように湯ぶねが並んでおり、見晴らしも最高、開放感にもあふれ、とてもいい温泉でした</th>\n",
       "      <th>バイキングの食事も品数、味付けともに良かった</th>\n",
       "      <th>杉乃井ホテルは横長の広いホテルなので、移動に時間がかかります</th>\n",
       "      <th>アクティブシニアの方もマイカーなしでリゾートライフを楽しむこともできます。</th>\n",
       "      <th>食事はバイキングですが、質が高いので、何を食べても満足でした</th>\n",
       "      <th>プールもあり、時間がいくらあっても足りないと思いました</th>\n",
       "      <th>海側のお部屋は景色はいいのですが、朝日がまぶしいことは知っておいた方がいいかもしれません</th>\n",
       "      <th>雨ですが、ボウリングなどをし、天気も気にせず楽しめました</th>\n",
       "      <th>寝室でもログハウスを満喫することができます</th>\n",
       "      <th>子どもが裸足で結局お部屋をウロウロしたのが気になりました</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.7</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      学生時代の友人7名で利用させて頂きました  ちょうど桜が咲いていて、別荘地内の桜もライトアップされていて、雰囲気良かったです  \\\n",
       "mean                   4.4                                       4.6   \n",
       "std                    0.8                                       0.7   \n",
       "\n",
       "      信州安曇野、穂高温泉郷に位置する本格ログハウスの貸別荘です  コテージ内はとても清潔感があって安心して泊まれました  \\\n",
       "mean                            2.5                         4.7   \n",
       "std                             1.0                         0.7   \n",
       "\n",
       "      インターから遠くもなく、コテージの近くにはコンビニや買い出しできるところもあるので、立地的にも満足でした  \\\n",
       "mean                                                4.4      \n",
       "std                                                 0.8      \n",
       "\n",
       "      趣味で一棟貸しのところをいろいろ利用していますがこちらの設備環境はとても清潔で周辺もとても静かなところで居心地が大変良かったです  \\\n",
       "mean                                                4.6                  \n",
       "std                                                 0.7                  \n",
       "\n",
       "      到着した日や翌日は雪の多い日でしたがスタッフの方が車や周辺の除雪をしてくれて大変助かりました  \\\n",
       "mean                                             4.4   \n",
       "std                                              1.0   \n",
       "\n",
       "      また時間があれば利用させていただきたいと思います  ちょうど紅葉が綺麗な季節に滞在できました  \\\n",
       "mean                       3.5                   4.4   \n",
       "std                        1.2                   0.6   \n",
       "\n",
       "      チェックインする前から、暖房がついていたりして、オーナーの気遣いを感じることができました  \\\n",
       "mean                                           4.7   \n",
       "std                                            0.5   \n",
       "\n",
       "                  ...               \\\n",
       "mean              ...                \n",
       "std               ...                \n",
       "\n",
       "      「棚湯」って何？　と思っていたが、屋上に棚田のように湯ぶねが並んでおり、見晴らしも最高、開放感にもあふれ、とてもいい温泉でした  \\\n",
       "mean                                                4.6                 \n",
       "std                                                 0.6                 \n",
       "\n",
       "      バイキングの食事も品数、味付けともに良かった  杉乃井ホテルは横長の広いホテルなので、移動に時間がかかります  \\\n",
       "mean                     4.3                             3.0   \n",
       "std                      0.7                             1.3   \n",
       "\n",
       "      アクティブシニアの方もマイカーなしでリゾートライフを楽しむこともできます。   食事はバイキングですが、質が高いので、何を食べても満足でした  \\\n",
       "mean                                     2.7                             4.4   \n",
       "std                                      1.3                             0.7   \n",
       "\n",
       "      プールもあり、時間がいくらあっても足りないと思いました  \\\n",
       "mean                          4.4   \n",
       "std                           0.6   \n",
       "\n",
       "      海側のお部屋は景色はいいのですが、朝日がまぶしいことは知っておいた方がいいかもしれません  \\\n",
       "mean                                           3.3   \n",
       "std                                            1.1   \n",
       "\n",
       "      雨ですが、ボウリングなどをし、天気も気にせず楽しめました  寝室でもログハウスを満喫することができます  \\\n",
       "mean                           4.3                    2.8   \n",
       "std                            0.6                    1.2   \n",
       "\n",
       "      子どもが裸足で結局お部屋をウロウロしたのが気になりました  \n",
       "mean                           3.9  \n",
       "std                            1.2  \n",
       "\n",
       "[2 rows x 50 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##これが文章ベクトル 入力データ\n",
    "word2vec_array0\n",
    "word2vec_array1\n",
    "word2vec_array2\n",
    "word2vec_array3\n",
    "word2vec_array4\n",
    "##これが正解データ\n",
    "keyword0\n",
    "keyword1\n",
    "keyword2\n",
    "keyword3\n",
    "keyword4\n",
    "\n",
    "keyword0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [文章ベクトル,正解データ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "data = [[],[],[]]\n",
    "for vec, label, row in zip(word2vec_array0, keyword0.loc[\"mean\"].values, samples0):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array1, keyword1.loc[\"mean\"].values, samples1):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array2, keyword2.loc[\"mean\"].values, samples2):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array3, keyword3.loc[\"mean\"].values, samples3):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "for vec, label, row in zip(word2vec_array4, keyword4.loc[\"mean\"].values, samples4):\n",
    "    data[0].append(np.array(vec))\n",
    "    data[1].append(label)\n",
    "    data[2].append(row)\n",
    "print(len(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vector in data[0]:\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力長は25に決める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 25\n",
    "input_data = []\n",
    "for vector in data[0]:\n",
    "    context_vectors = vector\n",
    "    while(len(context_vectors)<input_length):\n",
    "        context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#     print(context_vectors)\n",
    "    input_data.append(context_vectors)\n",
    "output_data = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "input_data = np.array(input_data)\n",
    "output_data = np.array(output_data)\n",
    "# train_input_data, val_input_data, train_output_data, val_output_data = train_test_split(input_data,output_data,train_size=0.8, test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "# print(input_data.shape)\n",
    "# print(output_data.shape)\n",
    "# print(len(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 25, 200) (250,) 250\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 256)               467968    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 468,225\n",
      "Trainable params: 468,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_train = int(len(input_data) * 0.8)\n",
    "N_validation = len(input_data) - N_train\n",
    "\n",
    "print(input_data.shape,output_data.shape, len(data[2]))\n",
    "X_train, X_validation, Y_train, Y_validation, row_train, row_validation = \\\n",
    "    train_test_split(input_data, output_data, data[2], test_size=N_validation)\n",
    "\n",
    "\n",
    "maxlen = 25\n",
    "'''\n",
    "モデル設定\n",
    "'''\n",
    "n_in = 200  \n",
    "n_hidden = 256\n",
    "n_out = 1\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_hidden,\n",
    "               kernel_initializer=weight_variable,\n",
    "               input_shape=(maxlen, n_in)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_out, kernel_initializer=weight_variable))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "# optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 50 samples\n",
      "Epoch 1/300\n",
      "200/200 [==============================] - 2s 8ms/step - loss: 15.0612 - val_loss: 10.3938\n",
      "Epoch 2/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 8.6113 - val_loss: 5.8527\n",
      "Epoch 3/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 5.1871 - val_loss: 4.1172\n",
      "Epoch 4/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 3.6856 - val_loss: 2.9461\n",
      "Epoch 5/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 2.6135 - val_loss: 2.1933\n",
      "Epoch 6/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 2.0385 - val_loss: 1.6580\n",
      "Epoch 7/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.5785 - val_loss: 1.2625\n",
      "Epoch 8/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 1.1418 - val_loss: 0.9713\n",
      "Epoch 9/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.9407 - val_loss: 0.7548\n",
      "Epoch 10/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.8708 - val_loss: 0.6067\n",
      "Epoch 11/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.7203 - val_loss: 0.5038\n",
      "Epoch 12/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6984 - val_loss: 0.4383\n",
      "Epoch 13/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6445 - val_loss: 0.4010\n",
      "Epoch 14/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6126 - val_loss: 0.3709\n",
      "Epoch 15/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.3499\n",
      "Epoch 16/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5735 - val_loss: 0.3375\n",
      "Epoch 17/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5675 - val_loss: 0.3358\n",
      "Epoch 18/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6156 - val_loss: 0.3348\n",
      "Epoch 19/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.3304\n",
      "Epoch 20/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5511 - val_loss: 0.3257\n",
      "Epoch 21/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5298 - val_loss: 0.3207\n",
      "Epoch 22/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.6034 - val_loss: 0.3263\n",
      "Epoch 23/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5977 - val_loss: 0.3240\n",
      "Epoch 24/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 0.3230\n",
      "Epoch 25/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5410 - val_loss: 0.3210\n",
      "Epoch 26/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.3319\n",
      "Epoch 27/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5670 - val_loss: 0.3155\n",
      "Epoch 28/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5668 - val_loss: 0.3154\n",
      "Epoch 29/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.3546\n",
      "Epoch 30/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4994 - val_loss: 0.3210\n",
      "Epoch 31/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 0.3213\n",
      "Epoch 32/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 0.3421\n",
      "Epoch 33/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.3153\n",
      "Epoch 34/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6087 - val_loss: 0.3026\n",
      "Epoch 35/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.3136\n",
      "Epoch 36/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5496 - val_loss: 0.3060\n",
      "Epoch 37/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.3196\n",
      "Epoch 38/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5722 - val_loss: 0.2901\n",
      "Epoch 39/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5269 - val_loss: 0.3032\n",
      "Epoch 40/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.5190 - val_loss: 0.3385\n",
      "Epoch 41/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5360 - val_loss: 0.2932\n",
      "Epoch 42/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4660 - val_loss: 0.3005\n",
      "Epoch 43/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.5231 - val_loss: 0.3292\n",
      "Epoch 44/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4985 - val_loss: 0.3175\n",
      "Epoch 45/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4833 - val_loss: 0.2863\n",
      "Epoch 46/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4322 - val_loss: 0.2591\n",
      "Epoch 47/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.2825\n",
      "Epoch 48/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4793 - val_loss: 0.2554\n",
      "Epoch 49/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3925 - val_loss: 0.2596\n",
      "Epoch 50/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4069 - val_loss: 0.4301\n",
      "Epoch 51/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4937 - val_loss: 0.3285\n",
      "Epoch 52/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4066 - val_loss: 0.2785\n",
      "Epoch 53/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3939 - val_loss: 0.2944\n",
      "Epoch 54/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3545 - val_loss: 0.2891\n",
      "Epoch 55/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.4029 - val_loss: 0.2782\n",
      "Epoch 56/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3704 - val_loss: 0.2771\n",
      "Epoch 57/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4062 - val_loss: 0.3152\n",
      "Epoch 58/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.4044 - val_loss: 0.2911\n",
      "Epoch 59/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2910 - val_loss: 0.2883\n",
      "Epoch 60/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.3915 - val_loss: 0.2735\n",
      "Epoch 61/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2862 - val_loss: 0.3733\n",
      "Epoch 62/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3039 - val_loss: 0.4162\n",
      "Epoch 63/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3065 - val_loss: 0.2836\n",
      "Epoch 64/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.2855 - val_loss: 0.2921\n",
      "Epoch 65/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2512 - val_loss: 0.3193\n",
      "Epoch 66/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3432 - val_loss: 0.3137\n",
      "Epoch 67/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2676 - val_loss: 0.2605\n",
      "Epoch 68/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2264 - val_loss: 0.2725\n",
      "Epoch 69/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2563 - val_loss: 0.4444\n",
      "Epoch 70/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2857 - val_loss: 0.3059\n",
      "Epoch 71/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2438 - val_loss: 0.3302\n",
      "Epoch 72/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2382 - val_loss: 0.2676\n",
      "Epoch 73/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2109 - val_loss: 0.2758\n",
      "Epoch 74/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2292 - val_loss: 0.2759\n",
      "Epoch 75/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2486 - val_loss: 0.3052\n",
      "Epoch 76/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2073 - val_loss: 0.2700\n",
      "Epoch 77/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2505 - val_loss: 0.2606\n",
      "Epoch 78/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1611 - val_loss: 0.2586\n",
      "Epoch 79/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1822 - val_loss: 0.2524\n",
      "Epoch 80/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2052 - val_loss: 0.2499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2364 - val_loss: 0.3667\n",
      "Epoch 82/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.3000 - val_loss: 0.3076\n",
      "Epoch 83/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1951 - val_loss: 0.2504\n",
      "Epoch 84/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1994 - val_loss: 0.2657\n",
      "Epoch 85/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1819 - val_loss: 0.2654\n",
      "Epoch 86/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1862 - val_loss: 0.2533\n",
      "Epoch 87/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2491 - val_loss: 0.2591\n",
      "Epoch 88/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2011 - val_loss: 0.3297\n",
      "Epoch 89/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2502 - val_loss: 0.3047\n",
      "Epoch 90/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2103 - val_loss: 0.2484\n",
      "Epoch 91/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1755 - val_loss: 0.2757\n",
      "Epoch 92/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2177 - val_loss: 0.2670\n",
      "Epoch 93/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2009 - val_loss: 0.2426\n",
      "Epoch 94/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1956 - val_loss: 0.2608\n",
      "Epoch 95/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1830 - val_loss: 0.2864\n",
      "Epoch 96/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1910 - val_loss: 0.2370\n",
      "Epoch 97/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1748 - val_loss: 0.3309\n",
      "Epoch 98/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2077 - val_loss: 0.2228\n",
      "Epoch 99/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1856 - val_loss: 0.2287\n",
      "Epoch 100/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1860 - val_loss: 0.2789\n",
      "Epoch 101/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2460 - val_loss: 0.4321\n",
      "Epoch 102/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2290 - val_loss: 0.2516\n",
      "Epoch 103/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1877 - val_loss: 0.2515\n",
      "Epoch 104/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1776 - val_loss: 0.2341\n",
      "Epoch 105/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1930 - val_loss: 0.2344\n",
      "Epoch 106/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1726 - val_loss: 0.2197\n",
      "Epoch 107/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2082 - val_loss: 0.3692\n",
      "Epoch 108/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2917 - val_loss: 0.2316\n",
      "Epoch 109/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1720 - val_loss: 0.2220\n",
      "Epoch 110/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1557 - val_loss: 0.2404\n",
      "Epoch 111/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1782 - val_loss: 0.2376\n",
      "Epoch 112/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2153 - val_loss: 0.2238\n",
      "Epoch 113/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1659 - val_loss: 0.2785\n",
      "Epoch 114/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2185 - val_loss: 0.2381\n",
      "Epoch 115/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1733 - val_loss: 0.2856\n",
      "Epoch 116/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1765 - val_loss: 0.2589\n",
      "Epoch 117/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1666 - val_loss: 0.2338\n",
      "Epoch 118/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1452 - val_loss: 0.2318\n",
      "Epoch 119/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2173 - val_loss: 0.2429\n",
      "Epoch 120/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1919 - val_loss: 0.2089\n",
      "Epoch 121/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2369 - val_loss: 0.2902\n",
      "Epoch 122/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2043 - val_loss: 0.2593\n",
      "Epoch 123/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1637 - val_loss: 0.2335\n",
      "Epoch 124/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.2569\n",
      "Epoch 125/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1644 - val_loss: 0.2162\n",
      "Epoch 126/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.2262\n",
      "Epoch 127/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2178 - val_loss: 0.4284\n",
      "Epoch 128/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2768 - val_loss: 0.2587\n",
      "Epoch 129/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1843 - val_loss: 0.2283\n",
      "Epoch 130/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1742 - val_loss: 0.2297\n",
      "Epoch 131/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.3175\n",
      "Epoch 132/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1621 - val_loss: 0.2657\n",
      "Epoch 133/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1786 - val_loss: 0.2127\n",
      "Epoch 134/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1859 - val_loss: 0.2363\n",
      "Epoch 135/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1564 - val_loss: 0.2333\n",
      "Epoch 136/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1586 - val_loss: 0.2159\n",
      "Epoch 137/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1659 - val_loss: 0.2125\n",
      "Epoch 138/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1837 - val_loss: 0.3375\n",
      "Epoch 139/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1857 - val_loss: 0.2647\n",
      "Epoch 140/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1475 - val_loss: 0.2285\n",
      "Epoch 141/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1573 - val_loss: 0.2618\n",
      "Epoch 142/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1765 - val_loss: 0.2480\n",
      "Epoch 143/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1645 - val_loss: 0.2156\n",
      "Epoch 144/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1650 - val_loss: 0.2067\n",
      "Epoch 145/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.2113\n",
      "Epoch 146/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1705 - val_loss: 0.3226\n",
      "Epoch 147/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2725 - val_loss: 0.3065\n",
      "Epoch 148/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1880 - val_loss: 0.2340\n",
      "Epoch 149/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1720 - val_loss: 0.2385\n",
      "Epoch 150/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2009 - val_loss: 0.2340\n",
      "Epoch 151/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1474 - val_loss: 0.2276\n",
      "Epoch 152/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1488 - val_loss: 0.2410\n",
      "Epoch 153/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.2246\n",
      "Epoch 154/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1569 - val_loss: 0.2329\n",
      "Epoch 155/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1618 - val_loss: 0.2357\n",
      "Epoch 156/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2045 - val_loss: 0.3024\n",
      "Epoch 157/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.2211\n",
      "Epoch 158/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.2629\n",
      "Epoch 159/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.2501\n",
      "Epoch 160/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1662 - val_loss: 0.2606\n",
      "Epoch 161/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1476 - val_loss: 0.2252\n",
      "Epoch 162/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1772 - val_loss: 0.2525\n",
      "Epoch 163/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.2504\n",
      "Epoch 164/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1867 - val_loss: 0.2239\n",
      "Epoch 165/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1661 - val_loss: 0.2639\n",
      "Epoch 166/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1617 - val_loss: 0.2603\n",
      "Epoch 167/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1869 - val_loss: 0.2581\n",
      "Epoch 168/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1701 - val_loss: 0.2537\n",
      "Epoch 169/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1442 - val_loss: 0.2329\n",
      "Epoch 170/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1741 - val_loss: 0.2848\n",
      "Epoch 171/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1235 - val_loss: 0.2488\n",
      "Epoch 172/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1461 - val_loss: 0.2506\n",
      "Epoch 173/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.2668\n",
      "Epoch 174/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1202 - val_loss: 0.2668\n",
      "Epoch 175/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2193 - val_loss: 0.3460\n",
      "Epoch 176/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1706 - val_loss: 0.2392\n",
      "Epoch 177/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1646 - val_loss: 0.2526\n",
      "Epoch 178/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1445 - val_loss: 0.2623\n",
      "Epoch 179/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1703 - val_loss: 0.2529\n",
      "Epoch 180/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1500 - val_loss: 0.2618\n",
      "Epoch 181/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1674 - val_loss: 0.2874\n",
      "Epoch 182/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1731 - val_loss: 0.2870\n",
      "Epoch 183/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1450 - val_loss: 0.2751\n",
      "Epoch 184/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1487 - val_loss: 0.2494\n",
      "Epoch 185/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1466 - val_loss: 0.2727\n",
      "Epoch 186/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1351 - val_loss: 0.2604\n",
      "Epoch 187/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1648 - val_loss: 0.3592\n",
      "Epoch 188/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1897 - val_loss: 0.2794\n",
      "Epoch 189/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1728 - val_loss: 0.2508\n",
      "Epoch 190/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1302 - val_loss: 0.2570\n",
      "Epoch 191/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1659 - val_loss: 0.2400\n",
      "Epoch 192/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1463 - val_loss: 0.2300\n",
      "Epoch 193/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1736 - val_loss: 0.2689\n",
      "Epoch 194/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1628 - val_loss: 0.2452\n",
      "Epoch 195/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1508 - val_loss: 0.2925\n",
      "Epoch 196/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1613 - val_loss: 0.2739\n",
      "Epoch 197/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1603 - val_loss: 0.3273\n",
      "Epoch 198/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1764 - val_loss: 0.2590\n",
      "Epoch 199/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1361 - val_loss: 0.2527\n",
      "Epoch 200/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1698 - val_loss: 0.2749\n",
      "Epoch 201/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1469 - val_loss: 0.2223\n",
      "Epoch 202/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1425 - val_loss: 0.2412\n",
      "Epoch 203/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1487 - val_loss: 0.2316\n",
      "Epoch 204/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1485 - val_loss: 0.2445\n",
      "Epoch 205/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1555 - val_loss: 0.2381\n",
      "Epoch 206/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1405 - val_loss: 0.2936\n",
      "Epoch 207/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.2065 - val_loss: 0.3181\n",
      "Epoch 208/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1737 - val_loss: 0.2338\n",
      "Epoch 209/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.2865\n",
      "Epoch 210/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1341 - val_loss: 0.2471\n",
      "Epoch 211/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.2433\n",
      "Epoch 212/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1365 - val_loss: 0.2423\n",
      "Epoch 213/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1441 - val_loss: 0.2388\n",
      "Epoch 214/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.2412\n",
      "Epoch 215/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1185 - val_loss: 0.2925\n",
      "Epoch 216/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.2593\n",
      "Epoch 217/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1704 - val_loss: 0.3049\n",
      "Epoch 218/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1928 - val_loss: 0.2568\n",
      "Epoch 219/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1451 - val_loss: 0.2335\n",
      "Epoch 220/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1396 - val_loss: 0.2242\n",
      "Epoch 221/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.2424\n",
      "Epoch 222/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1343 - val_loss: 0.2381\n",
      "Epoch 223/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1508 - val_loss: 0.2340\n",
      "Epoch 224/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.2911\n",
      "Epoch 225/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1909 - val_loss: 0.2650\n",
      "Epoch 226/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1529 - val_loss: 0.2489\n",
      "Epoch 227/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1151 - val_loss: 0.2719\n",
      "Epoch 228/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1495 - val_loss: 0.2782\n",
      "Epoch 229/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1164 - val_loss: 0.2869\n",
      "Epoch 230/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1300 - val_loss: 0.2813\n",
      "Epoch 231/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1500 - val_loss: 0.3017\n",
      "Epoch 232/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1328 - val_loss: 0.2476\n",
      "Epoch 233/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1485 - val_loss: 0.2745\n",
      "Epoch 234/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1332 - val_loss: 0.2348\n",
      "Epoch 235/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1033 - val_loss: 0.2372\n",
      "Epoch 236/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1140 - val_loss: 0.2401\n",
      "Epoch 237/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1196 - val_loss: 0.2356\n",
      "Epoch 238/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1440 - val_loss: 0.2337\n",
      "Epoch 239/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1843 - val_loss: 0.4003\n",
      "Epoch 240/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2426 - val_loss: 0.2489\n",
      "Epoch 241/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1406 - val_loss: 0.2439\n",
      "Epoch 242/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1313 - val_loss: 0.2437\n",
      "Epoch 243/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.2397\n",
      "Epoch 244/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1146 - val_loss: 0.2516\n",
      "Epoch 245/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1312 - val_loss: 0.2814\n",
      "Epoch 246/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.2425\n",
      "Epoch 247/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1063 - val_loss: 0.2655\n",
      "Epoch 248/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1471 - val_loss: 0.2939\n",
      "Epoch 249/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1265 - val_loss: 0.3130\n",
      "Epoch 250/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1452 - val_loss: 0.2722\n",
      "Epoch 251/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.2702\n",
      "Epoch 252/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1617 - val_loss: 0.2889\n",
      "Epoch 253/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1370 - val_loss: 0.2643\n",
      "Epoch 254/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.2503\n",
      "Epoch 255/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1287 - val_loss: 0.2289\n",
      "Epoch 256/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1306 - val_loss: 0.2422\n",
      "Epoch 257/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.2785\n",
      "Epoch 258/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.2511\n",
      "Epoch 259/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.2666\n",
      "Epoch 260/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1268 - val_loss: 0.2664\n",
      "Epoch 261/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1576 - val_loss: 0.2384\n",
      "Epoch 262/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0897 - val_loss: 0.2434\n",
      "Epoch 263/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1496 - val_loss: 0.2375\n",
      "Epoch 264/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1622 - val_loss: 0.2695\n",
      "Epoch 265/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1323 - val_loss: 0.2854\n",
      "Epoch 266/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1319 - val_loss: 0.2448\n",
      "Epoch 267/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.2416\n",
      "Epoch 268/300\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1279 - val_loss: 0.2573\n",
      "Epoch 269/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1299 - val_loss: 0.2891\n",
      "Epoch 270/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1756 - val_loss: 0.2993\n",
      "Epoch 271/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1208 - val_loss: 0.2342\n",
      "Epoch 272/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1149 - val_loss: 0.2382\n",
      "Epoch 273/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1288 - val_loss: 0.2404\n",
      "Epoch 274/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1047 - val_loss: 0.2461\n",
      "Epoch 275/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1279 - val_loss: 0.2772\n",
      "Epoch 276/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1243 - val_loss: 0.2601\n",
      "Epoch 277/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1379 - val_loss: 0.2559\n",
      "Epoch 278/300\n",
      "200/200 [==============================] - 1s 3ms/step - loss: 0.1334 - val_loss: 0.2573\n",
      "Epoch 279/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.2588\n",
      "Epoch 280/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.2989\n",
      "Epoch 281/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1538 - val_loss: 0.2613\n",
      "Epoch 282/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1454 - val_loss: 0.2683\n",
      "Epoch 283/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1121 - val_loss: 0.2534\n",
      "Epoch 284/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1388 - val_loss: 0.2848\n",
      "Epoch 285/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1327 - val_loss: 0.2425\n",
      "Epoch 286/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1312 - val_loss: 0.2897\n",
      "Epoch 287/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1230 - val_loss: 0.2836\n",
      "Epoch 288/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1213 - val_loss: 0.2835\n",
      "Epoch 289/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1116 - val_loss: 0.2571\n",
      "Epoch 290/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1191 - val_loss: 0.2638\n",
      "Epoch 291/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.2510\n",
      "Epoch 292/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1274 - val_loss: 0.3190\n",
      "Epoch 293/300\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.1546 - val_loss: 0.2844\n",
      "Epoch 294/300\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 0.1164"
     ]
    }
   ],
   "source": [
    "'''\n",
    "モデル学習\n",
    "'''\n",
    "epochs = 300\n",
    "batch_size = 100\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(X_validation, Y_validation),\n",
    "#           callbacks=[early_stopping])\n",
    "\n",
    "#earlystoppingなし\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_validation, Y_validation))\n",
    "\n",
    "\n",
    "# model.fit(X_train, Y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(X_validation, Y_validation))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "出力を用いて予測\n",
    "'''\n",
    "# 元データの最初の一部だけ切り出し\n",
    "# model.predict()\n",
    "# input_data[3:10].shape\n",
    "# word_model = word2vec.Word2Vec.load(\"./word2vec/model/wiki.model\")\n",
    "# test_text = \"非常 に 景色 と 温泉 が 綺麗 で 、 部屋 の 内装 や アメニティ が 良く 、 非常 に 満足 出来る もの で し た\"\n",
    "# test_vector = []\n",
    "# for char in test_text.split(\" \"):\n",
    "#     test_vector.append(word_model[char])\n",
    "\n",
    "# test_data = np.array([test_vector])\n",
    "# model.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input_length = 25\n",
    "# test_vectors = [] \n",
    "# len(data[0])\n",
    "# for v in data[0]:\n",
    "#     context_vectors = v\n",
    "#     if len(v) == 14:\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         context_vectors = np.append(context_vectors,[np.full(200,-1)], axis=0)\n",
    "#         test_vectors.append(context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vectors = np.array(test_vectors)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_validation_pred = model.predict(X_validation)\n",
    "# for text, pred in zip(row_train, y_train_pred):\n",
    "#     print(text, pred)\n",
    "\n",
    "for text, pred in zip(row_validation, y_validation_pred):\n",
    "    print(text, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intlen(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hidden256 dropout0.3が今の所一番精度が高い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adam epochs350が良さげ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### めも\n",
    "\n",
    "文章の情報量が対話継続欲求に影響して言えるのではないか。\n",
    "\n",
    "文章の情報量\n",
    "\n",
    "複数のモデルを作る\n",
    "\n",
    "\n",
    "綺麗でした　をそのまま学習したモデルと\n",
    "綺麗でした　を[0,2,0.0001,0,000002]情報量(単語頻度)で変換させた後学習したモデルを比較すれば情報量が影響しているかどうかがわかる\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
